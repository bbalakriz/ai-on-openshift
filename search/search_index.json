{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"demos/codellama-continue/codellama-continue/","title":"AI Assistant in Code Server","text":"<p>Who has never dreamed of having a know-it-all assistant that can help you writing boilerplate code, explain a function you got from a colleague but that remains obscure to you, or quickly parse through tons of error logs to fetch the one relevant line that can help you debug you application?</p> <p>With the advent of AI and LLM, such services have become pretty common (Copilot, CodeWhisperer,...). However, you may wonder what happens with all the code you are sending to those services? Or more than often, you're simply not allowed to use them...</p> <p>In this article, we will show you how to integrate a code assistant in Code Server, leveraging the CodeLlama model and the Continue plugin (an open source assistant for VSCode similar to Copilot).</p> <p>All those solutions are open source, you fully control how they are deployed and what they do, and your code will never leave your environment!</p>"},{"location":"demos/codellama-continue/codellama-continue/#requirements","title":"Requirements","text":"<ul> <li>A model that has been trained for Code Generation. Many are available. The Big Code Leaderboard will give you information about the latest and greatest models. In this demo, we are going to use CodeLlama-7b-Instruct-hf because it's good enough for a demo and fits in our available GPU. For large real-world production workloads, you would of course need to consider a bigger version of this model (13B, 70B), or another model.</li> <li>You must serve your model and have its inference endpoint. Refer to the LLM Serving section to learn how to deploy your model on RHOAI/ODH.</li> <li>A Code Server workbench image available in your RHOAI or ODH environment. Starting at RHOAI 2.8, this image is available out of the box. Prior to that, you would have to import it as a custom image, like this one.</li> <li>As Code Server is fully open source, it does not include the Microsoft Marketplace. So you must download the Continue extension file from the Open VSX Registry. Click on the Download button and select the Linux x64 version. You will get a file named <code>Continue.continue-0.9.91@linux-x64.vsix</code> (or whatever version you download).</li> </ul>"},{"location":"demos/codellama-continue/codellama-continue/#installation","title":"Installation","text":"<ul> <li> <p>Create and Start your Workbench based on Code Server:</p> <p></p> </li> <li> <p>Upload the extension file:</p> <p></p> </li> <li> <p>Once uploaded, Code server will try to open the file and complain about its size, just close the tab:1</p> <p></p> </li> <li> <p>From the Extension menu on the sidebar, click on the three dots on the top right and select Install from VSIX:</p> <p></p> </li> <li> <p>From the menu that opens, select the file you uploaded:</p> <p></p> </li> <li> <p>Wait a few seconds... The installation completes and you're greeted with a welcome message:</p> <p></p> </li> <li> <p>You can close all the tabs or follow the tutorial.</p> </li> </ul>"},{"location":"demos/codellama-continue/codellama-continue/#configuration","title":"Configuration","text":"<p>By default, Continue is configured with different providers and models to test. But of course we want to add our own inference endpoint to use the model we have deployed.</p> <ul> <li> <p>In the Sidebar, click on the new icon that was added at the bottom, the one from Continue (it may trigger an error about Headers not being defined, you can ignore it for now). At the bottom right of the Continue panel, click on the Gear icon to open the configuration file.</p> <p></p> </li> <li> <p>In the models section, add the following configuration (replace your inference endpoint with the right value, and eventually the name of your model):</p> <pre><code>{\n    \"title\": \"CodeLlama-7b\",\n    \"model\": \"codellama/CodeLlama-7b-Instruct-hf\",\n    \"apiBase\": \"https://your-inference-endpoint/v1/\",\n    \"completionOptions\": {\n    \"temperature\": 0.1,\n    \"topK\": 1,\n    \"topP\": 1,\n    \"presencePenalty\": 0,\n    \"frequencyPenalty\": 0\n    },\n    \"provider\": \"openai\",\n    \"apiKey\": \"none\"\n}\n</code></pre> </li> <li> <p>You can also remove the other pre-defined models if you don't want to use them. You should end up with something like this:</p> <p></p> </li> <li> <p>In the tabAutocompleteModel section, add the following configuration (replace your inference endpoint with the right value, and eventually the name of your model), and add/modify the options to your liking (see documentation for all possible values):</p> <pre><code>\"tabAutocompleteModel\": {\n    \"title\": \"CodeLlama-7b\",\n    \"model\": \"codellama/CodeLlama-7b-Instruct-hf\",\n    \"apiBase\": \"https://your-inference-endpoint/v1/\",\n    \"completionOptions\": {\n    \"temperature\": 0.1,\n    \"topK\": 1,\n    \"topP\": 1,\n    \"presencePenalty\": 0,\n    \"frequencyPenalty\": 0\n    },\n    \"provider\": \"openai\",\n    \"apiKey\": \"none\"\n},\n\"tabAutocompleteOptions\": {\n    \"useCopyBuffer\": false,\n    \"maxPromptTokens\": 1024,\n    \"prefixPercentage\": 0.5\n},\n</code></pre> </li> <li> <p>You can also disable telemetry by setting the parameter to false. You should end up with something like this:</p> <p></p> </li> <li> <p>Once the configuration is finished (file is auto-saved), you should see the model name available in the drop-down at the bottom of the Continue extension pane:</p> <p></p> </li> </ul> <p>Continue is now ready to use!</p>"},{"location":"demos/codellama-continue/codellama-continue/#usage","title":"Usage","text":"<p>The best way to learn how to use it is to read the short documentation and experiment.</p> <p>Here is a small example of what you can do: Fast Edit (ask for some code generation), then Tab completion (let the assistant suggest the next piece of code), then Explain or do something on the side:</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/","title":"Credit Card Fraud Detection Demo using MLFlow and Red Hat OpenShift AI","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#demo-description-architecture","title":"Demo Description &amp; Architecture","text":"<p>The goal of this demo is to demonstrate how RHOAI and MLFlow can be used together to build an end-to-end MLOps platform where we can:</p> <ul> <li>Build and train models in RHOAI</li> <li>Track and store those models with MLFlow</li> <li>Serve a model stored in MLFlow using RHOAI Model Serving (or MLFlow serving)</li> <li>Deploy a model application in OpenShift that runs sends data to the served model and displays the prediction</li> </ul> <p>The architecture looks like this: </p> <p>Description of each component:</p> <ul> <li>Data Set: The data set contains the data used for training and evaluating the model we will build in this demo.</li> <li>RHOAI Notebook: We will build and train the model using a Jupyter Notebook running in RHOAI.</li> <li>MLFlow Experiment tracking: We use MLFlow to track the parameters and metrics (such as accuracy, loss, etc) of a model training run. These runs can be grouped under different \"experiments\", making it easy to keep track of the runs.</li> <li>MLFlow Model registry: As we track the experiment we also store the trained model through MLFlow so we can easily version it and assign a stage to it (for example Staging, Production, Archive).</li> <li>S3 (ODF): This is where the models are stored and what the MLFlow model registry interfaces with. We use ODF (OpenShift Data Foundation) according to the MLFlow guide, but it can be replaced with another solution.</li> <li>RHOAI Model Serving: We recommend using RHOAI Model Serving for serving the model. It's based on ModelMesh and allows us to easily send requests to an endpoint for getting predictions.</li> <li>Application interface: This is the interface used to run predictions with the model. In our case, we will build a visual interface (interactive app) using Gradio and let it load the model from the MLFlow model registry.</li> </ul> <p>The model we will build is a Credit Card Fraud Detection model, which predicts if a credit card usage is fraudulent or not depending on a few parameters such as: distance from home and last transaction, purchase price compared to median, if it's from a retailer that already has been purchased from before, if the PIN number is used and if it's an online order or not.</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#deploying-the-demo","title":"Deploying the demo","text":""},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Have Red Hat OpenShift AI (RHOAI) running in a cluster</li> </ul> <p>Note</p> <p>Note: You can use Open Data Hub instead of RHOAI, but some instructions and screenshots may not apply</p> <ul> <li>Have MLFlow running in a cluster</li> </ul>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#11-mlflow-route-through-the-visual-interface","title":"1.1: MLFlow Route through the visual interface","text":"<p>Start by finding your route to MLFlow. You will need it to send any data to MLFlow.</p> <ul> <li>Go to the OpenShift Console as a Developer</li> <li>Select your mlflow project</li> <li>Press Topology</li> <li>Press the mlflow-server circle<ul> <li>While you are at it, you can also press the little \"Open URL\" button in the top right corner of the circle to open up the MLFlow UI in a new tab - we will need it later.</li> </ul> </li> <li>Go to the Resources tab</li> <li>Press mlflow-server under Services</li> <li>Look at the Hostname and mlflow-server Port.</li> </ul> <p>Note</p> <p>This route and port only work internally in the cluster.</p> <p> </p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#12-get-the-mlflow-route-using-command-line","title":"1.2: Get the MLFlow Route using command-line","text":"<p>Alternatively, you can use the OC command to get the hostname through: <code>oc get svc mlflow-server -n mlflow -o go-template --template='{{.metadata.name}}.{{.metadata.namespace}}.svc.cluster.local{{println}}'</code></p> <p>The port you will find with: <code>oc get svc mlflow-server -n mlflow -o yaml</code> </p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#2-create-a-rhoai-workbench","title":"2: Create a RHOAI workbench","text":"<p>Start by opening up RHOAI by clicking on the 9 square symbol in the top menu and choosing \"Red Hat OpenShift AI\".</p> <p></p> <p>Then create a new Data Science project (see image), this is where we will build and train our model. This will also create a namespace in OpenShift which is where we will be running our application after the model is done. I'm calling my project 'Credit Card Fraud', feel free to call yours something different but be aware that some things further down in the demo may change.</p> <p></p> <p>After the project has been created, create a workbench where we can run Jupyter. There are a few important settings here that we need to set:</p> <ul> <li>Name: Credit Fraud Model</li> <li>Notebook Image: Standard Data Science</li> <li>Deployment Size: Small</li> <li>Environment Variable: Add a new one that's a Config Map -&gt; Key/value and enter<ul> <li>Key: <code>MLFLOW_ROUTE</code></li> <li>Value: <code>http://&lt;route-to-mlflow&gt;:&lt;port&gt;</code>, replacing <code>&lt;route-to-mlflow&gt;</code> and <code>&lt;port&gt;</code> with the route and port that we found in step one.  In my case it is <code>http://mlflow-server.mlflow.svc.cluster.local:8080</code>.</li> </ul> </li> <li>Cluster Storage: Create new persistent storage - I call it \"Credit Fraud Storage\" and set the size to 20GB.</li> </ul> <p></p> <p>Press Create Workbench and wait for it to start - status should say \"Running\" and you should be able to press the Open link.</p> <p></p> <p>Open the workbench and login if needed.</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#3-train-the-model","title":"3: Train the model","text":"<p>When inside the workbench (Jupyter), we are going to clone a GitHub repository which contains everything we need to train (and run) our model. You can clone the GitHub repository by pressing the GitHub button in the left side menu (see image), then select \"Clone a Repository\" and enter this GitHub URL: https://github.com/red-hat-data-services/credit-fraud-detection-demo</p> <p></p> <p>Open up the folder that was added (credit-fraud-detection-demo). It contains:</p> <ul> <li>Data for training and evaluating the model.</li> <li>A notebook (model.ipynb) inside the <code>model</code> folder with a Deep Neural Network model we will train.</li> <li>An application (model_application.py) inside the <code>application</code> folder that will fetch the trained model from MLFlow and run a prediction on it whenever it gets any user input.</li> </ul> <p>The <code>model.ipynb</code> is what we are going to use for building and training the model, so open that up and take a look inside, there is documentation outlining what each cell does. What is particularly interesting for this demo are the last two cells.</p> <p>The second to last cell contains the code for setting up MLFlow tracking:</p> <pre><code>mlflow.set_tracking_uri(MLFLOW_ROUTE)\nmlflow.set_experiment(\"DNN-credit-card-fraud\")\nmlflow.tensorflow.autolog(registered_model_name=\"DNN-credit-card-fraud\")\n</code></pre> <p><code>mlflow.set_tracking_uri(MLFLOW_ROUTE)</code> just points to where we should send our MLFlow data. <code>mlflow.set_experiment(\"DNN-credit-card-fraud\")</code> tells MLFlow that we want to create an experiment, and what we are going to call it. In this case I call it \"DNN-credit-card-fraud\" as we are building a Deep Neural Network. <code>mlflow.tensorflow.autolog(registered_model_name=\"DNN-credit-card-fraud\")</code> enables autologging of a bunch of variables (such as accuracy, loss, etc) so we don't manually have to track them. It also automatically uploads the model to MLFlow after the training completes. Here we name the model the same as the experiment.</p> <p>Then in the last cell we have our training code:</p> <pre><code>with mlflow.start_run():\n    epochs = 2\n    history = model.fit(X_train, y_train, epochs=epochs, \\\n                        validation_data=(scaler.transform(X_val),y_val), \\\n                        verbose = True, class_weight = class_weights)\n\n    y_pred_temp = model.predict(scaler.transform(X_test))\n\n    threshold = 0.995\n\n    y_pred = np.where(y_pred_temp &gt; threshold, 1,0)\n    c_matrix = confusion_matrix(y_test,y_pred)\n    ax = sns.heatmap(c_matrix, annot=True, cbar=False, cmap='Blues')\n    ax.set_xlabel(\"Prediction\")\n    ax.set_ylabel(\"Actual\")\n    ax.set_title('Confusion Matrix')\n    plt.show()\n\n    t_n, f_p, f_n, t_p = c_matrix.ravel()\n    mlflow.log_metric(\"tn\", t_n)\n    mlflow.log_metric(\"fp\", f_p)\n    mlflow.log_metric(\"fn\", f_n)\n    mlflow.log_metric(\"tp\", t_p)\n\n    model_proto,_ = tf2onnx.convert.from_keras(model)\n    mlflow.onnx.log_model(model_proto, \"models\")\n</code></pre> <p><code>with mlflow.start_run():</code> is used to tell MLFlow that we are starting a run, and we wrap our training code with it to define exactly what code belongs to the \"run\". Most of the rest of the code in this cell is normal model training and evaluation code, but at the bottom we can see how we send some custom metrics to MLFlow through <code>mlflow.log_metric</code> and then convert the model to ONNX. This is because ONNX is one of the standard formats for RHOAI Model Serving which we will use later.</p> <p>Now run all the cells in the notebook from top to bottom, either by clicking Shift-Enter on every cell, or by going to Run-&gt;Run All Cells in the very top menu. If everything is set up correctly it will train the model and push both the run and the model to MLFlow. The run is a record with metrics of how the run went, while the model is the actual tensorflow and ONNX model which we later will use for inference. You may see some warnings in the last cell related to MLFlow, as long as you see a final progressbar for the model being pushed to MLFlow you are fine: </p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#4-view-the-model-in-mlflow","title":"4: View the model in MLFlow","text":"<p>Let's take a look at how it looks inside MLFlow now that we have trained the model. If you opened the MLFlow UI in a new tab in step 1.1, then just swap over to that tab, otherwise follow these steps:</p> <ul> <li>Go to the OpenShift Console</li> <li>Make sure you are in Developer view in the left menu</li> <li>Go to Topology in the left menu</li> <li>At the top left, change your project to \"mlflow\" (or whatever you called it when installing the MLFlow operator in pre-requisites)</li> <li>Press the \"Open URL\" icon in the top right of the MLFlow circle in the topology map</li> </ul> <p></p> <p>When inside the MLFlow interface you should see your new experiment in the left menu. Click on it to see all the runs under that experiment name, there should only be a single run from the model we just trained. You can now click on the row in the Created column to get more information about the run and how to use the model from MLFlow.</p> <p></p> <p>We will need the Full Path of the model in the next section when we are going to serve it, so keep this open.</p> <p></p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#5-serve-the-model","title":"5: Serve the model","text":"<p>Note</p> <p>You can either serve the model using RHOAI Model Serving or use the model straight from MLFlow. We will here show how you serve it with RHOAI Model Serving as that scales better for large applications and load. At the bottom of this section we'll go through how it would look like to use MLFlow instead.</p> <p>To start, go to your RHOAI Project and click \"Add data connection\". This data connection connects us to a storage we can load our models from.</p> <p></p> <p>Here we need to fill out a few details. These are all assuming that you set up MLFlow according to this guide and have it connected to ODF. If that's not the case then enter the relevant details for your use case.</p> <ul> <li>Name: mlflow-connection</li> <li>AWS_ACCESS_KEY_ID: Run <code>oc get secrets mlflow-server -n mlflow -o json | jq -r '.data.AWS_ACCESS_KEY_ID|@base64d'</code> in your command prompt, in my case it's <code>nB0z01i0PwD9PMSISQ2W</code></li> <li>AWS_SECRET_ACCESS_KEY: Run <code>oc get secrets mlflow-server -n mlflow -o json | jq -r '.data.AWS_SECRET_ACCESS_KEY|@base64d'</code> in your command prompt, in my case it's <code>FLgEJmGQm5CdRQRnXc8jVFcc+QDpM1lcrGpiPBzI</code>.</li> </ul> <p>Note</p> <p>In my case the cluster and storage has already been shut down, don't share this in normal cases.</p> <ul> <li>AWS_S3_ENDPOINT: Run <code>oc get configmap mlflow-server -n mlflow -o yaml | grep BUCKET_HOS</code> in your command prompt, in my case it's <code>http://s3.openshift-storage.svc</code></li> <li>AWS_DEFAULT_REGION: Where the cluster is being ran</li> <li>AWS_S3_BUCKET: Run <code>oc get obc -n mlflow -o yaml | grep bucketName</code> in your command prompt, in my case it's <code>mlflow-server-576a6525-cc5b-46cb-95f3-62c3986846df</code></li> </ul> <p>Then press \"Add data connection\". Here's an example of how it can look like: </p> <p>Then we will configure a model server, which will serve our models.</p> <p></p> <p>Just check the 'Make deployed available via an external route' checkbox and then press \"Configure\" at the bottom.</p> <p>Finally, we will deploy the model, to do that, press the \"Deploy model\" button which is in the same place that \"Configure Model\" was before. We need to fill out a few settings here:</p> <ul> <li>Name: credit card fraud</li> <li>Model framework: onnx-1 - Since we saved the model as ONNX in the model training section</li> <li>Model location:<ul> <li>Name: <code>mlflow-connection</code></li> <li>Folder path: This is the full path we can see in the MLFlow interface from the end of the previous section. In my case it's <code>1/b86481027f9b4b568c9efa3adc01929f/artifacts/models/</code>. Beware that we only need the last part, which looks something like: <code>1/..../artifacts/models/</code> </li> </ul> </li> </ul> <p></p> <p>Press Deploy and wait for it to complete. It will show a green checkmark when done. You can see the status here:</p> <p></p> <p>Click on \"Internal Service\" in the same row to see the endpoints, we will need those when we deploy the model application.</p> <p>[Optional] MLFlow Serving:</p> <p>This section is optional</p> <p>This section explains how to use MLFlow Serving instead of RHOAI Model Serving. We recommend using RHOAI Model Serving as it scales better. However, if you quickly want to get a model up and running for testing, this would be an easy way.</p> <p>To use MLFlow serving, simply deploy an application which loads the model straight from MLFlow. You can find the model application code for using MLFlow serving in the \"application_mlflow_serving\" folder in the GitHub repository you cloned in step 3.</p> <p>If you look inside <code>model_application_mlflow_serve.py</code> you are going to see a few particularly important lines of code:</p> <pre><code># Get a few environment variables. These are so we can:\n# - get data from MLFlow\n# - Set server name and port for Gradio\nMLFLOW_ROUTE = os.getenv(\"MLFLOW_ROUTE\")\n...\n\n# Connect to MLFlow using the route.\nmlflow.set_tracking_uri(MLFLOW_ROUTE)\n\n# Specify what model and version we want to load, and then load it.\nmodel_name = \"DNN-credit-card-fraud\"\nmodel_version = 1\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f\"models:/{model_name}/{model_version}\"\n)\n</code></pre> <p>Here is where we set up everything that's needed for loading the model from MLFlow. The environment variable MLFLOW_ROUTE is set in the Dockerfile. You can also see that we specifically load version 1 of the model called \"DNN-credit-card-fraud\" from MLFlow. This makes sense since we only ran the model once, but is easy to change if any other version or model should go into production</p> <p>Follow the steps of the next section to see how to deploy an application, but when given the choice for \"Context dir\" and \"Environment variables (runtime only)\", use these settings instead:</p> <ul> <li>Context dir: \"/model_application_mlflow_serve\"</li> <li>Environment variables (runtime only) fields:<ul> <li>Name: <code>MLFLOW_ROUTE</code></li> <li>Value: The MLFlow route from step one (<code>http://mlflow-server.mlflow.svc.cluster.local:8080</code> for example)</li> </ul> </li> </ul>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#6-deploy-the-model-application","title":"6: Deploy the model application","text":"<p>The model application is a visual interface for interacting with the model. You can use it to send data to the model and get a prediction of whether a transaction is fraudulent or not. You can find the model application code in the \"application\" folder in the GitHub repository you cloned in step 3.</p> <p></p> <p>If you look inside it <code>model_application.py</code>, you will see two particularly important lines of code:</p> <pre><code># Get a few environment variables. These are so we:\n# - Know what endpoint we should request\n# - Set server name and port for Gradio\nURL = os.getenv(\"INFERENCE_ENDPOINT\") &lt;----------\n...\n\n    response = requests.post(URL, json=payload, headers=headers)  &lt;----------\n</code></pre> <p>This is what we use to send a request to our RHOAI Model Server with some data we want it to run a prediction on.</p> <p>We are going to deploy the application with OpenShift by pointing to the GitHub repository. It will pull down the folder, automatically build a container image based on the Dockerfile, and publish it.</p> <p>To do this, go to the OpenShift Console and make sure you are in Developer view and have selected the credit-card-fraud project. Then press \"+Add\" in the left menu and select Import from Git.</p> <p></p> <p>In the \"Git Repo URL\" enter: https://github.com/red-hat-data-services/credit-fraud-detection-demo (this is the same repository we pulled into RHOAI earlier). Then press \"Show advanced Git options\" and set \"Context dir\" to \"/application\". Finally, at the very bottom, click the blue \"Deployment\" link:</p> <p></p> <p>Set these values in the Environment variables (runtime only) fields:</p> <ul> <li>Name: <code>INFERENCE_ENDPOINT</code></li> <li>Value: In the RHOAI projects interface (from the previous section), copy the \"restURL\" and add <code>/v2/models/credit-card-fraud/infer</code> to the end if it's not already there. For example: <code>http://modelmesh-serving.credit-card-fraud:8008/v2/models/credit-card-fraud/infer</code> </li> </ul> <p>Your full settings page should look something like this:</p> <p></p> <p>Press Create to start deploying the application.</p> <p>You should now see three objects in your topology map, one for the Workbench we created earlier, one for the model serving, and one for the application we just added. When the circle of your deployment turns dark blue it means that it has finished deploying.</p> <p>If you want more details on how the deployment is going, you can press the circle and look at Resources in the right menu that opens up. There you can see how the build is going and what's happening to the pod. The application will be ready when the build is complete and the pod is \"Running\".</p> <p>When the application has been deployed you can press the \"Open URL\" button to open up the interface in a new tab.</p> <p></p> <p>Congratulations, you now have an application running your AI model!</p> <p>Try entering a few values and see if it predicts it as a credit fraud or not. You can select one of the examples at the bottom of the application page.</p> <p></p>"},{"location":"demos/financial-fraud-detection/financial-fraud-detection/","title":"Financial Fraud Detection","text":"<p>Info</p> <p>The full source and instructions for this demo are available on this repo</p> <p>This demo shows how to use OpenShift AI to train and test a relatively simplistic fraud detection model. In exploring this content, you will become familiar with the OpenShift AI offering and common workflows to use with it.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/","title":"Chat with your Documentation","text":"<p>If you want to learn more about LLMs and how to serve them, please read the LLM Serving documentation first.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#rag-chatbot-full-walkthrough","title":"RAG Chatbot Full Walkthrough","text":"<p>Although the available code is normally pretty well documented, especially the notebooks, giving a full overview will surely help you understand how all of the different elements fit together.</p> <p>For this walkthrough we will be using this application, which is a RAG-based Chatbot that will use a Milvus vector store, vLLM for LLM serving, Langchain as the \"glue\" between those components, and Gradio as the UI engine.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#requirements","title":"Requirements","text":"<ul> <li>An OpenShift cluster with RHOAI or ODH deployed.</li> <li>A node with a GPU card. For the model we will use, 24GB memory on the GPU (VRAM) is necessary. If you have less than that you can either use quantization when loading the model, use an already quantized model (results may vary as they are not all compatible with the model server), or choose another compatible smaller model.</li> </ul>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#model-serving","title":"Model Serving","text":"<p>Deploy vLLM Model Serving instance in the OpenAI compatible API mode, either:</p> <ul> <li>as a custom server runtime in ODH/RHOAI.</li> <li>as a standalone server in OpenShift.</li> </ul> <p>In both cases, make sure you deploy the model <code>mistralai/Mistral-7B-Instruct-v0.2</code>.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#vector-store","title":"Vector Store","text":""},{"location":"demos/llm-chat-doc/llm-chat-doc/#milvus-deployment","title":"Milvus deployment","text":"<p>For our RAG we will need a Vector Database to store the Embeddings of the different documents. In this example we are using Milvus.</p> <p>Deployment instructions specific to OpenShift are available here.</p> <p>After you follow those instructions you should have a Milvus instance ready to be populated with documents.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#document-ingestion","title":"Document ingestion","text":"<p>In this notebook you will find detailed instructions on how to ingest different types of documents: PDFs first, then Web pages.</p> <p>The examples are based on RHOAI documentation, but of course we encourage you to use your own documentation. After all that's the purpose of all of this!</p> <p>This other notebook will allow you to execute simple queries against your Vector Store to make sure it works alright.</p> <p>Note</p> <p>Those notebooks are using the NomicAI Embeddings to create and query the collection. If you want to use the default embeddings from Langchain, other notebooks are available. They have the same name, just without the <code>-nomic</code> at the end.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#testing","title":"Testing","text":"<p>Now let's put all of this together!</p> <p>This notebook will be used to create a RAG solution leveraging the LLM and the Vector Store we just populated. Don't forget to enter the relevant information about your Model Server (the Inference URL and model name), and about your Vector store (connection information and collection name) on the third cell.</p> <p>You can also adjust other parameters as you see fit.</p> <p></p> <ul> <li>It will first initialize a connection to the vector database (embeddings are necessary for the Retriever to \"understand\" what is stored in the database):</li> </ul> <pre><code>model_kwargs = {'trust_remote_code': True}\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"nomic-ai/nomic-embed-text-v1\",\n    model_kwargs=model_kwargs,\n    show_progress=False\n)\n</code></pre> <ul> <li>A prompt template is then defined. You can see that we will give it specific instructions on how the model must answer. This is necessary if you want to keep it focused on its task and not say anything that may not be appropriate (on top of getting you fired!). The format of this prompt is originally the one used for Llama2, but Mistral uses the same one. You may have to adapt this format if you use another model.</li> </ul> <pre><code>template=\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful and honest assistant named HatBot answering questions.\nYou will be given a question you need to answer, and a context to provide you with information. You must answer the question based as much as possible on this context.\nAlways answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n&lt;&lt;/SYS&gt;&gt;\n\nContext: \n{context}\n\nQuestion: {question} [/INST]\n\"\"\"\n</code></pre> <ul> <li>Now we will define the LLM connection itself. As you can see there are many parameters you can define that will modify how the model will answer. Details on those parameters are available here.</li> </ul> <pre><code>llm =  VLLMOpenAI(\n    openai_api_key=\"EMPTY\",\n    openai_api_base=INFERENCE_SERVER_URL,\n    model_name=MODEL_NAME,\n    max_tokens=MAX_TOKENS,\n    top_p=TOP_P,\n    temperature=TEMPERATURE,\n    presence_penalty=PRESENCE_PENALTY,\n    streaming=True,\n    verbose=False,\n    callbacks=[StreamingStdOutCallbackHandler()]\n)\n</code></pre> <ul> <li>And finally we can tie it all together with a specific chain, RetrievalQA:</li> </ul> <pre><code>qa_chain = RetrievalQA.from_chain_type(\n        llm,\n        retriever=store.as_retriever(\n            search_type=\"similarity\",\n            search_kwargs={\"k\": 4}\n            ),\n        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n        return_source_documents=True\n        )\n</code></pre> <ul> <li>That's it! We can now use this chain to send queries. The retriever will look for relevant documents in the Vector Store, their content will be injected automatically in the prompt, and the LLM will try to create a valid answer based on its own knowledge and this content:</li> </ul> <pre><code>question = \"How can I create a Data Science Project?\"\nresult = qa_chain.invoke({\"query\": question})\n</code></pre> <ul> <li>The last cell in the notebook will simply filter for duplicates in the sources that were returned in the <code>result</code>, and display them:</li> </ul> <pre><code>def remove_duplicates(input_list):\n    unique_list = []\n    for item in input_list:\n        if item.metadata['source'] not in unique_list:\n            unique_list.append(item.metadata['source'])\n    return unique_list\n\nresults = remove_duplicates(result['source_documents'])\n\nfor s in results:\n    print(s)\n</code></pre>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#application","title":"Application","text":"<p>Notebooks are great and everything, but it's not what you want to show to your users. I hope...</p> <p>So instead, here is a simple UI that implements mostly the same code we used in the notebooks.</p> <p></p> <p>The deployment is already explained in the repo and pretty straightforward as the application will only \"consume\" the same Vector Store and LLM Serving we have used in the notebooks. However I will point out some specificities:</p> <ul> <li>This implementation allows you to have different collections in Milvus you can query from. This is fully configurable, you can create as many collections as you want and add them to the application.</li> <li>The code is more complicated than the notebooks as it allows for multiple users to use the application simultaneously. They can all use a different collection, ask questions at the same time, they stay fully isolated. The limitation is the memory you have.</li> <li>Most (if not all) parameters are configurable. They are all described in the README file.</li> </ul> <p>Some info on the code itself (<code>app.py</code>):</p> <ul> <li><code>load_dotenv</code>, along with the <code>env.example</code> file (once renamed <code>.env</code>) will allow you to develop locally.</li> <li>As normally your Milvus instance won't be exposed externally to OpenShift, if you want to develop locally you may want to open a tunnel to it with <code>oc port-forward Service/milvus-service 19530:19530</code> (replace with the name of the Milvus Service along with the ports if you change them). You can use the same technique for the LLM endpoint if you have not exposed it as a route.</li> <li>The class <code>QueueCallback</code> was necessary because the <code>vLLMOpenAI</code> library used to query the model does not return an iterator in the format Langchain expects it (at the time of this writing). Instead, this implementation of the Callback functions for the LLM puts the new tokens in a Queue (L43) that is then retrieved from continuously (L65), with the content being yielded for display. This is a little bit convoluted, but the whole stack is still in full development, so sometimes you have to be creative...</li> <li>The default Milvus Retriever (same for almost all vector databases in Langchain) does not allow to filter on the score. This means that whatever your query, some documents will always be fetched and passed into the context. This is an unwanted behavior if the query has no relation to the knowledge base you are using. So I created a custom Retriever Class, in the file <code>milvus_retriever_with_score_threshold.py</code> that allows to filter the documents according to score. NOTE: this a similarity search with a cosine score, so the lesser, the better. The threshold calculation is \"no more than...\".</li> <li>Gradio configuration is pretty straightforward trough the ChatInterface component, only hiding some buttons, adding an avatar image for the bot,... The only notable thing is the use of a State variable for the selected collection so that a switch from one collection to the other is not applied to all users (this is an early mistake I made \ud83d\ude0a) .</li> </ul> <p>Here is what you RAG-based Chatbot should look like:</p>"},{"location":"demos/retail-object-detection/retail-object-detection/","title":"Object Detection in Retail","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p> <p>In this demo, you can see how to build an intelligent application that gives a customer the ability to find merchandise discounts, for shirts, as they browse clothing in a department store.</p> <p>You can download the related presentation.</p> <p></p> <p></p> <p></p>"},{"location":"demos/smart-city/smart-city/","title":"Smart City, an Edge-to-Core Data Story","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p> <p>In this demo, we show how to implement this scenario:</p> <ul> <li>Using a trained ML model, licence plates are recognized at toll location.</li> <li>Data (plate number, location, timestamp) is send from toll locations (edge) to the core using Kafka mirroring to handle communication issues and recovery.</li> <li>Incoming data is screened real-time to trigger alerts for wanted vehicles (Amber Alert).</li> <li>Data is aggregated and stored into object storage.</li> <li>A central database contains other information coming from licence registry system: car model, color,\u2026\u200b</li> <li>Data analysis leveraging Presto and Superset is done against stored data.</li> </ul> <p>This demo is showcased in this video.</p> <p></p> <p></p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/","title":"Telecom Customer Churn using Airflow and Red Hat OpenShift AI","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#demo-description","title":"Demo description","text":"<p>The goal of this demo is to demonstrate how Red Hat OpenShift AI (RHOAI) and Airflow can be used together to build an easy-to-manage pipeline. To do that, we will show how to build and deploy an airflow pipeline, mainly with Elyra but also some tips if you want to build it manually. In the end, you will have a pipeline that:</p> <ul> <li>Loads some data</li> <li>Trains two different models</li> <li>Evaluates which model is best</li> <li>Saves that model to S3</li> </ul> <p>Hint</p> <p>You can expand on this demo by loading the pushed model into MLFlow, or automatically deploying it into some application, like in the Credit Card Fraud Demo</p> <p>The models we build are used to predict customer churn for a Telecom company using structured data. The data contains fields such as: If they are a senior citizen, if they are a partner, their tenure, etc.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#deploying-the-demo","title":"Deploying the demo","text":""},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Fork this git repository into a GitHub or GitLab repo (the demo shows steps for GitHub, but either works): https://github.com/red-hat-data-services/telecom-customer-churn-airflow</li> <li>Have Airflow running in a cluster and point Airflow to the cloned git repository.</li> <li>Have access to some S3 storage (this guide uses ODF with a bucket created in the namespace \"airflow\").</li> <li>Have Red Hat OpenShift AI (RHOAI) running in a cluster. Make sure you have admin access in RHOAI, or know someone who does.</li> </ul> <p>Note</p> <p>Note: You can use Open Data Hub instead of RHOAI, but some instructions and screenshots may not apply</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#1-open-up-airflow","title":"1: Open up Airflow","text":"<p>You find the route to the Airflow console through this command: <code>oc get route -n airflow</code></p> <p></p> <p>Enter it in the browser and you will see something like this:</p> <p></p> <p>Keep that open in a tab as we will come back to Airflow later on.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#2-add-elyra-as-a-custom-notebook-image","title":"2: Add Elyra as a Custom Notebook Image","text":"<p>It's possible to build pipelines by creating an Airflow DAG script in python. Another, arguably simpler, method is to use Elyra to visually build out the pipeline and then submit it to Airflow. Most of this demo is going to be revolving around using Elyra together with Airflow, but at the very end, there will be a bonus section for how to use Airflow independently.</p> <p>To get access to Elyra, we will simply import it as a custom notebook image. Start by opening up RHOAI by clicking on the 9-square symbol in the top menu and choosing \"Red Hat OpenShift AI\".</p> <p></p> <p>Then go to Settings -&gt; Notebook Images and press \"Import new image\". If you can't see Settings then you are lacking sufficient access. Ask your admin to add this image instead.</p> <p></p> <p>Under Repository enter: <code>quay.io/eformat/elyra-base:0.2.1</code> and then name it something like <code>Elyra</code>.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#3-create-a-rhoai-workbench","title":"3: Create a RHOAI workbench","text":"<p>A workbench in RHOAI lets us spin up and down notebooks as needed and bundle them under Projects, which is a great way to get easy access to compute resources and keep track of your work. Start by creating a new Data Science project (see image). I'm calling my project 'Telecom Customer Churn', feel free to call yours something different but be aware that some things further down in the demo may change.</p> <p></p> <p>After the project has been created, create a workbench where we can run Jupyter. There are a few important settings here that we need to set:</p> <ul> <li>Name: Customer Churn</li> <li>Notebook Image: Elyra</li> <li>Deployment Size: Small</li> <li>Environment Variables: Secret -&gt; AWS with your AWS details</li> </ul> <p></p> <p>Press Create Workbench and wait for it to start - status should say \"Running\" and you should be able to press the Open link.</p> <p></p> <p>Open the workbench and login if needed.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#4-load-a-git-repository","title":"4: Load a Git repository","text":"<p>When inside the workbench (Jupyter), we are going to clone a GitHub repository that contains everything we need to build our DAG. You can clone the GitHub repository by pressing the GitHub button in the left side menu (see image), then select \"Clone a Repository\" and enter your GitHub URL (Your forked version of this: https://github.com/red-hat-data-services/telecom-customer-churn-airflow)</p> <p></p> <p>The notebooks we will use are inside the <code>include/notebooks</code> folder, there should be 5 in total, 4 for building the pipeline and 1 for verifying that everything worked. They all run standard Python code, which is the beauty of Airflow combined with Elyra. There is no need to worry about additional syntax.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#5-configure-elyra-to-work-with-airflow","title":"5: Configure Elyra to work with Airflow","text":"<p>Before we can build and run any DAGs through Elyra, we first need to configure Elyra to talk with our Airflow instance. There will be two ways to configure this, either visually or through the terminal. Chose one for each section. If you want to do it through the terminal, then open the terminal like this: </p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#51-create-a-runtime-image","title":"5.1 Create a Runtime Image","text":"<p>We will start by configuring a Runtime Image, this is the image we will use to run each node in our pipeline. Open Runtime Images on the left-hand side of the screen.  </p> <p></p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#511-create-the-runtime-image-visually","title":"5.1.1 Create the Runtime Image visually","text":"<p>Press the plus icon next to the Runtime Images title to start creating a new Runtime Image. There are only three fields we need to worry about here:</p> <ul> <li>Display name: <code>airflow-runner</code></li> <li>Image Name: <code>quay.io/eformat/airflow-runner:2.5.1</code></li> <li>Image Pull Policy: Always</li> </ul> <p></p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#512-create-the-runtime-image-via-the-terminal","title":"5.1.2 Create the Runtime Image via the terminal","text":"<p>Execute this in the terminal:</p> <pre><code>mkdir -p ~/.local/share/jupyter/metadata/runtime-images/\ncat &lt;&lt; EOF &gt; ~/.local/share/jupyter/metadata/runtime-images/airflow-runner.json\n{\n  \"display_name\": \"airflow-runner\",\n  \"metadata\": {\n    \"tags\": [],\n    \"display_name\": \"airflow-runner\",\n    \"image_name\": \"quay.io/eformat/airflow-runner:2.5.1\",\n    \"pull_policy\": \"Always\"\n  },\n  \"schema_name\": \"runtime-image\"\n}\nEOF\n</code></pre> <p>Refresh and you should see <code>airflow-runner</code> appear in the Runtime Images.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#52-create-a-runtime","title":"5.2 Create a Runtime","text":"<p>Now we just need a Runtime configuration, which is what Elyra will use to save the DAG (in our Git repo), connect to Airflow and run the pipeline. Just like with the Runtime image, we can configure this visually or via the terminal.</p> <p>Open Runtimes on the left-hand side of the screen.  </p> <p></p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#521-configure-the-runtime-visually","title":"5.2.1 Configure the Runtime visually","text":"<p>Press the plus icon next to the title, select \"New Apache Airflow runtime configuration\" and enter these fields:</p> <p></p> <p>General settings:</p> <ul> <li>Display Name: <code>airflow</code></li> </ul> <p>Airflow settings:</p> <ul> <li>Apache Airflow UI Endpoint: run <code>oc get route -n airflow</code> to get the route</li> <li>Apache Airflow User Namespace: <code>airflow</code></li> </ul> <p>Github/GitLabs settings:</p> <ul> <li>Git type: GITHUB or GITLAB, depending on where you stored the repository</li> <li>GitHub or GitLab server API Endpoint: <code>https://api.github.com</code> or your GitLab endpoint</li> <li>GitHub or GitLab DAG Repository: Your repository (<code>red-hat-data-services/telecom-customer-churn-airflow</code> in my case)</li> <li>GitHub or GitLab DAG Repository Branch: Your branch (<code>main</code> in my case)</li> <li>Personal Access Token: A personal access token for pushing to the repository</li> </ul> <p>Cloud Object Storage settings: These completely depend on where and how you set up your S3 storage. If you created a bucket from ODF then it will look similar to this:</p> <ul> <li>Cloud Object Storage Endpoint: <code>http://s3.openshift-storage.svc</code></li> <li>Cloud Object Storage Bucket Name: The name of your bucket (<code>airflow-storage-729b10d1-f44d-451d-badb-fbd140418763</code> in my case)</li> <li>Cloud Object Storage Authentication Type: KUBERNETES_SECRET</li> <li>Cloud Object Storage Credentials Secret: The name of your secret containing the access and secret key is (in my case it was <code>airflow-storage</code>, which is the name I gave the Object Bucket Claim)</li> <li>Cloud Object Storage Username: your AWS_ACCESS_KEY_ID</li> <li>Cloud Object Storage Password: your AWS_SECRET_ACCESS_KEY</li> </ul>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#522-configure-the-runtime-via-the-terminal","title":"5.2.2 Configure the Runtime via the terminal","text":"<p>If you prefer doing this through the terminal, then execute this in the terminal and replace any variables with their values (see the visual section for hints):</p> <pre><code>mkdir -p ~/.local/share/jupyter/metadata/runtimes\ncat &lt;&lt; EOF &gt;  ~/.local/share/jupyter/metadata/runtimes/airflow.json\n{\n  \"display_name\": \"airflow\",\n  \"metadata\": {\n     \"tags\": [],\n     \"display_name\": \"airflow\",\n     \"user_namespace\": \"airflow\",\n     \"git_type\": \"GITHUB\",\n     \"github_api_endpoint\": \"https://${GIT_SERVER}\",\n     \"api_endpoint\": \"${AIRFLOW_ROUTE}\",\n     \"github_repo\": \"${GIT_REPO}\",\n     \"github_branch\": \"main\",\n     \"github_repo_token\": \"${GIT_TOKEN}\",\n     \"cos_auth_type\": \"KUBERNETES_SECRET\",\n     \"cos_endpoint\": \"${STORAGE_ENDPOINT}\",\n     \"cos_bucket\": \"${STORAGE_BUCKET}\",\n     \"cos_secret\": \"airflow-storage\" - the name of your secret,\n     \"cos_username\": \"${AWS_ACCESS_KEY_ID}\",\n     \"cos_password\": \"${AWS_SECRET_ACCESS_KEY}\",\n     \"runtime_type\": \"APACHE_AIRFLOW\"\n  },\n  \"schema_name\": \"airflow\"\n}\nEOF\n</code></pre> <p>Refresh and you should see <code>airflow</code> appear in the Runtimes.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#6-create-a-dag-with-elyra","title":"6. Create a DAG with Elyra","text":"<p>Now that we have a runtime and runtime image defined, we can build and run the pipeline. You can also find this pipeline in <code>/dags/train_and_compare_models.pipeline</code> if you prefer to just open an existing one.  </p> <p>To start creating a new pipeline, open up the launcher (click on the plus next to a notebook tab if you don't have it open), and press the \"Apache Airflow Pipeline Editor\". </p> <p>Now drag the Notebooks in the correct order and connect them up with each other. You can find the Notebooks in <code>/included/notebooks</code> and the correct order is: process_data -&gt; model_gradient_boost &amp; model_randomforest -&gt; compare_and_push. These are their functions:</p> <ul> <li>process_data.ipynb: Downloads data from GitHub that we will use to train the models. Then processes it, splits it into training and testing partitions and finally pushes it to S3.</li> <li>model_gradient_boost.ipynb: Fetches the processed data from S3 and uses it to train the model and evaluate it to get a test accuracy. Then pushes the model and the accompanying accuracy to S3.</li> <li>model_randomforest.ipynb: Fetches the processed data from S3 and uses it to train the model and evaluate it to get a test accuracy. Then pushes the model and the accompanying accuracy to S3.</li> <li>compare_and_push.ipynb: Downloads the models and their accuracies from S3, does a simple compare on which performs better, and pushes that model under the name \"best_model\" to S3.</li> </ul> <p></p> <p>After the notebooks are added, we need to go through each of them and change their Runtime Images to <code>airflow-runner</code> that we created earlier. </p> <p>We also need to set some environment variables so that the airflow nodes get access to the bucket name and endpoint when running, without hard-coding it in the notebooks. These details are already added to the Airflow Runtime we set up before, but when running it only passes along the Kubernetes secret which contains AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.  </p> <p>Add these two environment variables (both should be the same as you entered in section 5.2):</p> <ul> <li>Endpoint:<ul> <li>Name: AWS_S3_ENDPOINT</li> <li>Value: <code>http://s3.openshift-storage.svc</code> (or similar endpoint address)</li> </ul> </li> <li>Bucket name:<ul> <li>Name: AWS_S3_BUCKET</li> <li>Value: The name of your bucket (<code>airflow-storage-729b10d1-f44d-451d-badb-fbd140418763</code> in my case)</li> </ul> </li> </ul> <p></p> <p>Press Run to start the pipeline: </p> <p>You can now go to the Airflow UI to see the progress. If you have closed the tab then refer to section 1.</p> <p>In Airflow you will see a dag called <code>train_and_compare_models</code> with some numbers behind it. Click on it and go open the Graph tab. </p> <p></p> <p>If all are dark green that means that the run has completed successfully.</p> <p>We can now also confirm that the trained model was saved in our bucket by going back to the RHOAI notebook and running the notebook <code>test_airflow_success.ipynb</code>. If all went well it should print the model, its type and its accuracy. </p> <p>And that's how you can use Airflow together with RHOAI to create a pipeline!</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#bonus-section-use-an-airflow-dag-file","title":"Bonus section: Use an Airflow DAG file","text":"<p>Instead of building a pipeline through notebooks in Elyra, we can of course build and use an Airflow DAG. You can develop individual methods (data processing, mode training, etc) in RHOAI notebooks and then pull them all together in a DAG python file. This is a more segmented way for a Data Scientist to work than with Elyra, but still very possible within OpenShift and provides some more flexibility.  </p> <p>I have created a simple <code>test_dag.py</code> just to show what it can look like. You can find it in the <code>/dags</code> folder. Then it's up to you what operators you want to run, which secrets you want to load, etc. For inspiration, you can open up the automatically created Elyra DAG we just ran. To do that, go into the DAG and press Code:</p> <p></p> <p>Some notes if you wish to manually build a similar DAG:</p> <ul> <li>Make sure to add the environment variables</li> <li>Don't hardcode secrets into the DAG, but rather reference a Kubernetes secret. For example:</li> </ul> <pre><code>secrets=[\n        Secret(\"env\", \"AWS_ACCESS_KEY_ID\", \"airflow-storage\", \"AWS_ACCESS_KEY_ID\"),\n        Secret(\n            \"env\", \"AWS_SECRET_ACCESS_KEY\", \"airflow-storage\", \"AWS_SECRET_ACCESS_KEY\"\n        ),\n    ]\n</code></pre> <ul> <li>The image that is being used for the KubernetesPodOperator is <code>quay.io/eformat/airflow-runner:2.5.1</code></li> <li>If you want to run notebooks manually, look at the Papermill Operator</li> </ul>"},{"location":"demos/water-pump-failure-prediction/water-pump-failure-prediction/","title":"Water Pump Failure Prediction","text":"<p>Info</p> <p>The full source for this demo is available in this repo. Look in the <code>workshop</code> folder for the full instructions.</p> <p>This demo shows how to do detection of anomalies in sensor data. This web app allows you to broadcast various sources of data in real time.</p> <p></p>"},{"location":"demos/xray-pipeline/xray-pipeline/","title":"XRay Analysis Automated Pipeline","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p> <p>In this demo, we implement an automated data pipeline for chest Xray analysis:</p> <ul> <li>Ingest chest Xrays into an object store based on Ceph.</li> <li>The Object store sends notifications to a Kafka topic.</li> <li>A KNative Eventing Listener to the topic triggers a KNative Serving function.</li> <li>An ML-trained model running in a container makes a risk of Pneumonia assessment for incoming images.</li> <li>A Grafana dashboard displays the pipeline in real time, along with images incoming, processed and anonymized, as well as full metrics.</li> </ul> <p>This pipeline is showcased in this video (slides are also here).</p> <p></p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/","title":"Model Training and Serving - YOLOv5","text":"<p>Info</p> <p>The full source and instructions for this demo are available in these repos:</p> <ul> <li>Model Training</li> <li>Model Serving</li> </ul> <p>In this tutorial, we're going to see how you can customize YOLOv5, an object detection model, to recognize specific objects in pictures, and how to deploy and use this model.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#yolo-and-yolov5","title":"YOLO and YOLOv5","text":"<p>YOLO (You Only Look Once) is a popular object detection and image segmentation model developed by Joseph Redmon and Ali Farhadi at the University of Washington. The first version of YOLO was released in 2015 and quickly gained popularity due to its high speed and accuracy.</p> <p>YOLOv2 was released in 2016 and improved upon the original model by incorporating batch normalization, anchor boxes, and dimension clusters. YOLOv3 was released in 2018 and further improved the model's performance by using a more efficient backbone network, adding a feature pyramid, and making use of focal loss.</p> <p>In 2020, YOLOv4 was released which introduced a number of innovations such as the use of Mosaic data augmentation, a new anchor-free detection head, and a new loss function.</p> <p>In 2021, Ultralytics released YOLOv5, which further improved the model's performance and added new features such as support for panoptic segmentation and object tracking.</p> <p>YOLO has been widely used in a variety of applications, including autonomous vehicles, security and surveillance, and medical imaging. It has also been used to win several competitions, such as the COCO Object Detection Challenge and the DOTA Object Detection Challenge.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#model-training","title":"Model training","text":"<p>YOLOv5 has already been trained to recognize some objects. Here we are going to use a technique called Transfer Learning to adjust YOLOv5 to recognize a custom set of images.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#transfer-learning","title":"Transfer Learning","text":"<p>Transfer learning is a machine learning technique in which a model trained on one task is repurposed or adapted to another related task. Instead of training a new model from scratch, transfer learning allows the use of a pre-trained model as a starting point, which can significantly reduce the amount of data and computing resources needed for training.</p> <p>The idea behind transfer learning is that the knowledge gained by a model while solving one task can be applied to a new task, provided that the two tasks are similar in some way. By leveraging pre-trained models, transfer learning has become a powerful tool for solving a wide range of problems in various domains, including natural language processing, computer vision, and speech recognition.</p> <p>Ultralytics have fully integrated the transfer learning process in YOLOv5, making it easy for us to do. Let's go!</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#environment-and-prerequisites","title":"Environment and prerequisites","text":"<ul> <li>This training should be done in a Data Science Project to be able to modify the Workbench configuration (see below the /dev/shm issue).</li> <li>YOLOv5 is using PyTorch, so in RHOAI it's better to start with a notebook image already including this library, rather than having to install it afterwards.</li> <li>PyTorch is internally using shared memory (/dev/shm) to exchange data between its internal worker processes. However, default container engine configurations limit this memory to the bare minimum, which can make the process exhaust this memory and crash. The solution is to manually increase this memory by mounting a specific volume with enough space at this emplacement. This problem will be fixed in an upcoming version. Meanwhile you can use this procedure.</li> <li>Finally, a GPU is strongly recommended for this type of training.</li> </ul>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#data-preparation","title":"Data Preparation","text":"<p>To train the model we will of course need some data. In this case a sufficient number of images for the various classes we want to recognize, along with their labels and the definitions of the bounding boxes for the object we want to detect.</p> <p>In this example we will use images from Google's Open Images. We will work with 3 classes: Bicycle, Car and Traffic sign.</p> <p>We have selected only a few classes in this example to speed up the process, but of course feel free to adapt and choose the ones you want.</p> <p>For this first step:</p> <ul> <li>If not already done, create your Data Science Project,</li> <li>Create a Workbench of type PyTorch, with at least 8Gi of memory, 1 GPU and 20GB of storage.</li> <li>Apply this procedure to increase shared memory.</li> <li>Start the workbench.</li> <li>Clone the repository https://github.com/rh-aiservices-bu/yolov5-transfer-learning, open the notebook 01-data_preparation.ipynb and follow the instructions.</li> </ul> <p>Once you have completed to whole notebook the Dataset is ready for training!</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#training","title":"Training","text":"<p>In this example, we will do the training with the smallest base model available to save some time. Of course you can change this base model and adapt the various hyperparameters of the training to improve the result.</p> <p>For this second step, from the same workbench environment, open the notebook <code>02-model_training.ipynb</code> and follow the instructions.</p> <p>Warning</p> <p>The amount of memory you have assigned to your Workbench has a great impact on the batch size you will be able to work with, independently of the size of your GPU. For example, a batch size of 128 will barely fit into an 8Gi of memory Pod. The higher the better, until it breaks... Which you will find out soon anyway, after the first 1-2 epochs.</p> <p>Note</p> <p>During the training, you can launch and access Tensorboard by:</p> <ul> <li>Opening a Terminal tab in Jupyter</li> <li>Launch Tensorboard from this terminal with <code>tensorboard --logdir yolov5/runs/train</code></li> <li>Access Tensorboard in your browser using the same Route as your notebook, but replacing the <code>.../lab/...</code> part by <code>.../proxy/6006/</code>. Example: <code>https://yolov5-yolo.apps.cluster-address/notebook/yolo/yolov5/proxy/6006/</code></li> </ul> <p>Once you have completed to whole notebook you have a model that is able to recognize the three different classes on a given image.</p> <p></p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#model-serving","title":"Model Serving","text":"<p>We are going to serve a YOLOv5 model using the ONNX format, a general purpose open format built to represent machine learning models. RHOAI Model Serving includes the OpenVino serving runtime that accepts two formats for models: OpenVino IR, its own format, and ONNX.</p> <p>Note</p> <p>Many files and code we are going to use, especially the ones from the utils and models folders, come directly from the YOLOv5 repository. They includes many utilities and functions needed for image pre-processing and post-processing. We kept only what is needed, rearranged in a way easier to follow within notebooks. YOLOv5 includes many different tools and CLI commands that are worth learning, so don't hesitate to have a look at it directly.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#environment-and-prerequisites_1","title":"Environment and prerequisites","text":"<ul> <li>YOLOv5 is using PyTorch, so in RHOAI it's better to start with a notebook image already including this library, rather than having to install it afterwards.</li> <li>Although not necessary as in this example we won't use the model we trained in the previous section, the same environment can totally be reused.</li> </ul>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#converting-a-yolov5-model-to-onnx","title":"Converting a YOLOv5 model to ONNX","text":"<p>YOLOv5 is based on PyTorch. So base YOLOv5 models, or the ones you retrain using this framework, will come in the form of a <code>model.pt</code> file. We will first need to convert it to the ONNX format.</p> <ul> <li>From your workbench, clone the repository https://github.com/rh-aiservices-bu/yolov5-model-serving.</li> <li>Open the notebook <code>01-yolov5_to_onnx.ipynb</code> and follow the instructions.</li> <li>The notebook will guide you through all the steps for the conversion. If you don't want to do it at this time, you can also find in this repo the original YOLOv5 \"nano\" model, <code>yolov5n.pt</code>, and its already converted ONNX version, <code>yolov5n.onnx</code>.</li> </ul> <p>Once converted, you can save/upload your ONNX model to the storage you will use in your Data Connection on RHOAI. At the moment it has to be an S3-Compatible Object Storage, and the model must be in it own folder (not at the root of the bucket).</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#serving-the-model","title":"Serving the model","text":"<p>Here we can use the standard configuration path for RHOAI Model Serving:</p> <ul> <li>Create a Data Connection to the storage where you saved your model. In this example we don't need to expose an external Route, but of course you can. In this case though, you won't be able to directly see the internal gRPC and REST endpoints in the RHOAI UI, you will have to get them from the Network-&gt;Services panel in the OpenShift Console.</li> <li>Create a Model Server, then deploy the model using the ONNX format.</li> </ul> <p>Note</p> <p>You can find full detailed versions of this procedure in this Learning Path or in the RHOAI documentation.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#grpc-connection","title":"gRPC connection","text":"<p>With the gRPC interface of the model server, you have access to different Services. They are described, along with their format, in the <code>grpc_predict_v2.proto</code> file.</p> <p>There are lots of important information in this file: how to query the service, how to format the data,... This is really important as the data format is not something you can \"invent\", and not exactly the same compared as the REST interface (!).</p> <p>This proto file, which is a service description meant to be used with any programming language, has already been converted to usable Python modules defining objects and classes to be used to interact with the service: <code>grpc_predict_v2_pb2.py</code> and <code>grpc_predict_v2_pb2_grpc.py</code>. If you want to learn more about this, the conversion can be done using the protoc tool.</p> <p>You can use the notebook <code>02-grpc.ipynb</code> to connect to the interface and test some of the services. You will see that many \"possible\" services from ModelMesh are unfortunately simply not implemented with the OpenVino backend at the time of this writing. But at least ModelMetadata will give some information on the formats we have to use for inputs and outputs when doing the inference.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#consuming-the-model-over-grpc","title":"Consuming the model over gRPC","text":"<p>In the <code>03-remote_inference_grpc.ipynb</code> notebook, you will find a full example on how to query the grpc endpoint to make an inference. It is backed by the file <code>remote_infer_grpc.py</code>, where most of the relevant code is:</p> <ul> <li>Image preprocessing on L35: reads the image and transforms it in a proper numpy array</li> <li>gRPC request content building on L44: transforms the array in the expected input shape (refer to model metadata obtained in the previous notebook), then flatten it as expected by ModelMesh.</li> <li>gRPC calling on L58.</li> <li>Response processing on L73: reshape the response from flat array to expected output shape (refer to model metadata obtained in the previous notebook), run NMS to remove overlapping boxes, draw the boxes from results.</li> </ul> <p>The notebook gives the example for one image, as well as the processing of several ones from the <code>images</code> folder. This allows for a small benchmark on processing/inference time.</p> <p></p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#consuming-the-model-over-rest","title":"Consuming the model over REST","text":"<p>In the <code>04-remote_inference_rest.ipynb</code> notebook, you will find a full example on how to query the gRPC endpoint to make an inference. It is backed by the file <code>remote_infer_rest.py</code>, where most of the relevant code is:</p> <ul> <li>Image preprocessing on L30: reads the image and transforms it in a proper numpy array</li> <li>Payload building on L39: transforms the array in the expected input shape (refer to model metadata obtained in the previous notebook).</li> <li>REST calling on L54.</li> <li>Response processing on L60: reshape the response from flat array to expected output shape (refer to model metadata obtained in the previous notebook), run NMS to remove overlapping boxes, draw the boxes from results.</li> </ul> <p>The notebook gives the example for one image, as well as the processing of several ones from the <code>images</code> folder. This allows for a small benchmark on processing/inference time.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#grpc-vs-rest","title":"gRPC vs REST","text":"<p>Here are a few elements to help you choose between the two available interfaces to query your model:</p> <ul> <li>REST is easier to implement: it is a much better known protocol for most people, and involves a little bit less programming. There is no need to create a connection, instantiate objects,... So it's often easier to use.</li> <li>If you want to query the model directly from outside OpenShift, you have to use REST which is the only one exposed. You can expose gRPC too, but it's kind of difficult right now.</li> <li>gRPC is wwwwwaaaayyyyy much faster than REST. With the exact same model serving instance, as showed in the notebooks, inferences are about 30x faster. That is huge when you have score of images to process.</li> </ul>"},{"location":"generative-ai/llm-serving/","title":"LLM Serving","text":"<p>Info</p> <p>All source files and examples used in this article are available on this repo!</p> <p>LLMs (Large Language Models) are the subject of the day. And of course, you can definitely work with them on OpenShift with ODH or RHOAI, from creating a Chatbot, using them as simple APIs to summarize or translate texts, to deploying a full application that will allow you to quickly query your documentation or knowledge base in natural language.</p> <p>You will find on this page instructions and examples on how to set up the different elements that are needed for those different use cases, as well as fully implemented and ready-to-use applications.</p>"},{"location":"generative-ai/llm-serving/#context-and-definitions","title":"Context and definitions","text":"<p>Many people are only beginning to discover those technologies. After all, it has been a little bit more than a year since the general public is aware of them, and many related technologies, tools or applications are only a few months, even weeks (and sometimes days!) old. So let's start with a few definitions of the different terms that will be used in this article.</p> <ul> <li>LLM: A Large Language Model (LLM) is a sophisticated artificial intelligence system designed for natural language processing. It leverages deep learning techniques to understand and generate human-like text. LLMs use vast datasets to learn language patterns, enabling tasks like text generation, translation, summarization, and more. These models are versatile and can be fine-tuned for specific applications, like chatbots or content creation. LLMs have wide-ranging potential in various industries, from customer support and content generation to research and education, but their use also raises concerns about ethics, bias, and data privacy, necessitating responsible deployment and ongoing research.</li> <li>Fine-tuning: Fine-tuning in the context of Large Language Models (LLMs) is a process of adapting a pre-trained, general-purpose model to perform specific tasks or cater to particular applications. It involves training the model on a narrower dataset related to the desired task, allowing it to specialize and improve performance. Fine-tuning customizes the LLM's capabilities for tasks like sentiment analysis, question answering, or chatbots. This process involves adjusting hyperparameters, data preprocessing, and possibly modifying the model architecture. Fine-tuning enables LLMs to be more effective and efficient in specific domains, extending their utility across various applications while preserving their initial language understanding capabilities.</li> <li>RAG: RAG, or Retrieval-Augmented Generation, is a framework in natural language processing. It combines two key components: retrieval and generation. Retrieval involves selecting relevant information from a vast knowledge base, like the internet, and generation pertains to creating human-like text. RAG models employ a retriever to fetch context and facts related to a specific query or topic and a generator, often a language model, to produce coherent responses. This approach enhances the quality and relevance of generated text, making it useful for tasks like question answering, content summarization, and information synthesis, offering a versatile solution for leveraging external knowledge in AI-powered language understanding and production.</li> <li>Embeddings: Embeddings refer to a technique in natural language processing and machine learning where words, phrases, or entities are represented as multi-dimensional vectors in a continuous vector space. These vectors capture semantic relationships and similarities between words based on their context and usage. Embeddings are created through unsupervised learning, often using models like Word2Vec or GloVe, which transform words into fixed-length numerical representations. These representations enable machines to better understand and process language, as similar words have closer vector representations, allowing algorithms to learn contextual associations. Embeddings are foundational in tasks like text classification, sentiment analysis, machine translation, and recommendation systems.</li> <li>Vector Database: A vector database is a type of database designed to efficiently store and manage vector data, which represents information as multidimensional arrays or vectors. Unlike traditional relational databases, which organize data in structured tables, vector databases excel at handling unstructured or semi-structured data. They are well-suited for applications in data science, machine learning, and spatial data analysis, as they enable efficient storage, retrieval, and manipulation of high-dimensional data points. Vector databases play a crucial role in various fields, such as recommendation systems, image processing, natural language processing, and geospatial analysis, by facilitating complex mathematical operations on vector data for insights and decision-making.</li> <li>Quantization: Model quantization is a technique in machine learning and deep learning aimed at reducing the computational and memory requirements of neural networks. It involves converting high-precision model parameters (usually 32-bit floating-point values) into lower precision formats (typically 8-bit integers or even binary values). This process helps in compressing the model, making it more lightweight and faster to execute on hardware with limited resources, such as edge devices or mobile phones. Quantization can result in some loss of model accuracy, but it's a trade-off that balances efficiency with performance, enabling the deployment of deep learning models in resource-constrained environments without significant sacrifices in functionality.</li> </ul> <p>Fun fact: all those definitions were generated by an LLM...</p> <p>Do you want to know more?</p> <p>Here are a few worth reading articles:</p> <ul> <li>Best article ever: A jargon-free explanation of how AI large language models work</li> <li>Understanding LLama2 and its architecture</li> <li>RAG vs Fine-Tuning, which is best?</li> </ul>"},{"location":"generative-ai/llm-serving/#llm-serving_1","title":"LLM Serving","text":"<p>LLM Serving is not a trivial task, at least in a production environment...</p> <p></p> <ul> <li>LLMs are usually huge (several GBs, tens of GBs...) and require GPU(s) with enough memory if you want decent accuracy and performance. Granted, you can run smaller models on home hardware with good results, but that's not the subject here. After all we are on OpenShift, so more in a large organization environment than in an enthusiastic programmer basement!</li> <li>A served LLM will generally be used by multiple applications and users simultaneously. Since you can't just throw resources at it and scale your infrastructure easily because of the previous point, you want to optimize response time by for example batching queries, caching or buffering them,... Those are special operations that have to be handled specifically.</li> <li>When you load an LLM, there are parameters you want to tweak at load time, so a \"generic\" model loader is not the best suited solution.</li> </ul>"},{"location":"generative-ai/llm-serving/#llm-serving-solutions","title":"LLM Serving solutions","text":"<p>Fortunately, we have different solutions to handle LLM Serving.</p> <p>On this repo you will find:</p> <ul> <li>Recipes to deploy different types of LLM Servers, either using the Single Stack Model Serving available in ODH and RHOAI, or as Standalone deployments.</li> <li>Notebook examples on how to query those different servers are available.</li> </ul>"},{"location":"generative-ai/llm-serving/#serving-runtimes-for-single-stack-model-serving","title":"Serving Runtimes for Single Stack Model Serving","text":"<p>On top of the Caikit+TGIS or TGIS built-in runtimes, the following custom runtimes can be imported in the Single-Model Serving stack of Open Data Hub or OpenShift AI.</p> <ul> <li>vLLM Serving Runtime</li> <li>Hugging Face Text Generation Inference</li> </ul>"},{"location":"generative-ai/llm-serving/#standalone-inference-servers","title":"Standalone Inference Servers","text":"<ul> <li>vLLM: how to deploy vLLM, the \"Easy, fast, and cheap LLM serving for everyone\".</li> <li>Hugging Face TGI: how to deploy the Text Generation Inference server from Hugging Face.</li> <li>Caikit-TGIS-Serving: how to deploy the Caikit-TGIS-Serving stack, from OpenDataHub.</li> </ul>"},{"location":"generative-ai/llm-serving/#what-are-the-differences","title":"What are the differences?","text":"<p>You should read the documentation of those different model servers, as they present different characteristics:</p> <ul> <li>Different acceleration features can be implemented from one solution to the other. Depending on the model you choose, some features may be required.</li> <li>The endpoints can be accessed in REST or gRPC mode, or both, depending on the server.</li> <li>APIs are different, with some solutions offering an OpenAI compatible, which simplifies the integration with some libraries like Langchain.</li> </ul>"},{"location":"generative-ai/llm-serving/#which-model-to-use","title":"Which model to use?","text":"<p>In this section we will assume that you want to work with a \"local\" open source model, and not consume a commercial one through an API, like OpenAI's ChatGPT or Anthropic's Claude.</p> <p>There are literally hundreds of thousands of models available, almost all of them available on the Hugging Face site. If you don't know what this site is, you can think of it as what Quay or DockerHub are for containers: a big repository of models and datasets ready to download and use. Of course Hugging Face (the company) is also creating code, providing hosting capabilities,... but that's another story.</p> <p>So which model to choose will depend on several factors:</p> <ul> <li>Of course how good this model is. There are several benchmarks that have been published, as well as constantly updated rankings.</li> <li>The dataset it was trained on. Was it curated or just raw data from anywhere, does it contain nsfw material,...? And of course what the license is (some datasets are provided for research only or non-commercial).</li> <li>The license of the model itself. Some are fully open source, some claim to be... They may be free to use in most cases, but have some restrictions attached to them (looking at you Llama2...).</li> <li>The size of the model. Unfortunately that may be the most restrictive point for your choice. The model simply must fit on the hardware you have at your disposal, or the amount of money you are willing to pay.</li> </ul> <p>Currently, a good LLM with interesting performance for a relatively small size is Mistral-7B. Fully Open Source with an Apache 2.0 license, it will fit in an unquantized version on about 22GB of VRAM, which is perfect for an A10G card.</p> <p>Embeddings are another type of model often associated with LLMs are they are used to convert documented into vectors. A database of those vectors can then be queries to find the documents related to a query you make. This is very useful for the RAG solution we are going to talk about later on. NomicAI recently released a really performant and fully open source embeddings model, nomic-embed-text-v1</p>"},{"location":"generative-ai/llm-serving/#llm-consumption","title":"LLM Consumption","text":"<p>Once served, consuming an LLM is pretty straightforward, as at the end of the day it's only an API call.</p> <p>You can always query those models directly through a curl command or a simple request using Python. However, for easier consumption and integration with other tools, a few libraries/SDKs are available to streamline the process. They will allow you to easily connect to Vector Databases or Search Agents, chain multiple models, tweak parameters,... in a few lines of code. The main libraries at the time of this writing are Langchain, LlamaIndex and Haystack.</p> <p>In the LLM on OpenShift examples section repo, you will find several notebooks and full UI examples that will show you how to use those libraries with different types of model servers to create your own Chatbot!</p>"},{"location":"generative-ai/what-is-generative-ai/","title":"What is Generative AI?","text":"<p>Generative AI generally has the following characteristics:</p> <ul> <li>A model generally created based on a vast and broad set of data.</li> <li>Aims to generate novel content or data</li> <li>by mimicking patterns found in the training data</li> <li>e.g. \"compose an email to let the customer know that their loan application has been denied\"</li> </ul>"},{"location":"getting-started/opendatahub/","title":"What is Open Data Hub?","text":"<p>Open Data Hub (ODH) is an open source project that provides open source AI tools for running large and distributed AI workloads on the OpenShift Container Platform. Currently, the Open Data Hub project provides open source tools for distributed AI and Machine Learning (ML) workflows, Jupyter Notebook development environment and monitoring. The Open Data Hub project roadmap offers a view on new tools and integration the project developers are planning to add.</p> <p></p> <p>Included in the Open Data Hub core deployment is several open source components, which can be individually enabled. They include:</p> <ul> <li>Jupyter Notebooks</li> <li>ODH Dashboard</li> <li>Data Science Pipelines</li> <li>Model Mesh Serving</li> </ul> <p>Want to know more?</p>"},{"location":"getting-started/openshift-ai/","title":"OpenShift AI","text":""},{"location":"getting-started/openshift-ai/#what-is-red-hat-openshift-ai","title":"What is Red Hat OpenShift AI?","text":"<p>Red Hat OpenShift AI (RHOAI) builds on the capabilities of Red Hat OpenShift to provide a single, consistent, enterprise-ready hybrid AI and MLOps platform. It provides tools across the full lifecycle of AI/ML experiments and models including training, serving, monitoring, and managing AI/ML models and AI-enabled applications.</p> <p></p> <p>Note</p> <p>Red Hat OpenShift AI was previously named Red Hat OpenShift Data Science. Some texts or screenshots may not yet reflect this change.</p> <p>Documentation for Self-Managed RHOAI</p> <p>Documentation for Managed RHOAI</p> <p>Want to know more?</p>"},{"location":"getting-started/openshift/","title":"OpenShift and AI","text":""},{"location":"getting-started/openshift/#what-is-red-hat-openshift","title":"What is Red Hat OpenShift?","text":"<p>Red Hat OpenShift brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. Built on Kubernetes, it delivers a consistent experience across public cloud, on-premise, hybrid cloud, or edge architecture. Choose a self-managed or fully managed solution. No matter how you run it, OpenShift helps teams focus on the work that matters.</p> <p>Want to know more?</p>"},{"location":"getting-started/openshift/#why-ai-on-openshift","title":"Why AI on OpenShift?","text":"<p>AI/ML on OpenShift accelerates AI/ML workflows and the delivery of AI-powered intelligent application.</p>"},{"location":"getting-started/openshift/#mlops-with-red-hat-openshift","title":"MLOps with Red Hat OpenShift","text":"<p>Red Hat OpenShift includes key capabilities to enable machine learning operations (MLOps) in a consistent way across datacenters, public cloud computing, and edge computing.</p> <p>By applying DevOps and GitOps principles, organizations automate and simplify the iterative process of integrating ML models into software development processes, production rollout, monitoring, retraining, and redeployment for continued prediction accuracy.</p> <p>Learn more</p>"},{"location":"getting-started/openshift/#what-is-a-ml-lifecycle","title":"What is a ML lifecycle?","text":"<p>A multi-phase process to obtain the power of large volumes and a variety of data, abundant compute, and open source machine learning tools to build intelligent applications.</p> <p>At a high level, there are four steps in the lifecycle:</p> <ol> <li>Gather and prepare data to make sure the input data is complete, and of high quality</li> <li>Develop model, including training, testing, and selection of the model with the highest prediction accuracy</li> <li>Integrate models in application development process, and inferencing</li> <li>Model monitoring and management, to measure business performance and address potential production data drift</li> </ol> <p>On this site, you will find recipes, patterns, demos for various AI/ML tools and applications used through those steps.</p>"},{"location":"getting-started/openshift/#why-use-containers-and-kubernetes-for-your-machine-learning-initiatives","title":"Why use containers and Kubernetes for your machine learning initiatives?","text":"<p>Containers and Kubernetes are key to accelerating the ML lifecycle as these technologies provide data scientists the much needed agility, flexibility, portability, and scalability to train, test, and deploy ML models.</p> <p>Red Hat\u00ae OpenShift\u00ae is the industry's leading containers and Kubernetes hybrid cloud platform. It provides all these benefits, and through the integrated DevOps capabilities (e.g. OpenShift Pipelines, OpenShift GitOps, and Red Hat Quay) and integration with hardware accelerators, it enables better collaboration between data scientists and software developers, and accelerates the roll out of intelligent applications across hybrid cloud (data center, edge, and public clouds).</p>"},{"location":"getting-started/why-this-site/","title":"Why this site?","text":"<p>As data scientists and engineers, it's easy to find detailed documentation on the tools and libraries we use. But what about end-to-end data pipeline solutions that involve multiple products? Unfortunately, those resources can be harder to come by. Open source communities often don't have the resources to create and maintain them. But don't worry, that's where this website comes in!</p> <p>We've created a one-stop-shop for data practitioners to find recipes, reusable patterns, and actionable demos for building AI/ML solutions on OpenShift. And the best part? It's a community-driven resource site! So, feel free to ask questions, make feature requests, file issues, and even submit PRs to help us improve the content. Together, we can make data pipeline solutions easier to find and implement.</p> <p></p>"},{"location":"odh-rhoai/accelerator-profiles/","title":"Accelerator Profiles","text":""},{"location":"odh-rhoai/accelerator-profiles/#accelerator-profiles","title":"Accelerator Profiles","text":"<p>To effectively use accelerators in OpenShift AI, OpenShift AI Administrators need to create and manage associated accelerator profiles. </p> <p>An accelerator profile is a custom resource definition (CRD) that defines specifications for this accelerator. It can be direclty managed via the OpenShift AI Dashboard under Settings \u2192 Accelerator profiles.</p> <p>When working with GPU nodes in OpenShift AI, it is essential to set proper taints on those nodes. This prevents unwanted workloads from being scheduled on them when they don't have specific tolerations set. Those tolerations are configured in the accelerator profiles associated with each type of GPU, then applied to the workloads (Workbenches, Model servers,...) for which you have selected an accelerator profile.</p> <p>The taints in the GPU Worker Nodes should be set like this:</p> <pre><code>  taints:\n    - effect: NoSchedule\n      key: nvidia.com/gpu\n      value: NVIDIA-A10G-SHARED\n</code></pre> <p>A corresponding Accelerator profile can then be created to allow workloads to run on this type of node (in this example, nodes having an A10G GPU). Workloads that use another accelerator profile (for another type of GPU for example) or that don't have any Accelerator profile set will not be scheduled on nodes tainted with NVIDIA-A10G-SHARED.</p> <p>For a detailed guide on configuring and managing accelerator profiles in OpenShift AI, refer to our repository.</p>"},{"location":"odh-rhoai/configuration/","title":"ODH and RHOAI Configuration","text":""},{"location":"odh-rhoai/configuration/#standard-configuration","title":"Standard configuration","text":"<p>As an administrator of ODH/RHOAI, you have access to different settings through the Settings menu on the dashboard:</p> <p></p>"},{"location":"odh-rhoai/configuration/#custom-notebook-images","title":"Custom notebook images","text":"<p>This is where you can import other notebook images. You will find resources on available custom images and learn how to create your own in the Custom Notebooks section.</p> <p>To import a new image, follow those steps.</p> <ul> <li>Click on import image.</li> </ul> <p></p> <ul> <li>Enter the full address of your container, set a name (this is what will appear in the launcher), and a description.</li> </ul> <p></p> <ul> <li>On the bottom part, add information regarding the software and the packages that are present in this image. This is purely informative.</li> </ul> <p></p> <ul> <li>Your image is now listed and enabled. You can hide it without removing it by simply disabling it.</li> </ul> <p></p> <ul> <li>It is now available in the launcher, as well as in the Data Science Projects.</li> </ul> <p></p>"},{"location":"odh-rhoai/configuration/#cluster-settings","title":"Cluster settings","text":"<p>In this panel, you can adjust:</p> <ul> <li>The default size of the volumes created for new users.</li> <li>Whether you want to stop idle notebooks and, if so, after how much time.</li> </ul> <p>Note</p> <p>This feature currently looks at running Jupyter kernels, like a Python notebook. If you are only using a Terminal, or another IDE window like VSCode or RStudio from the custom images, this activity is not detected and your Pod can be stopped without notice after the set delay.</p> <ul> <li>Whether you allow usage data to be collected and reported.</li> <li>Whether you want to add a toleration to the notebook pods to allow them to be scheduled on tainted nodes. That feature is really useful if you want to dedicate specific worker nodes to running notebooks. Tainting them will prevent other workloads from running on them. Of course, you have to add the toleration to the pods.</li> </ul> <p></p>"},{"location":"odh-rhoai/configuration/#user-management","title":"User management","text":"<p>In this panel, you can edit who has access to RHOAI by defining the \"Data Science user groups\", and who has access to the Settings by defining the \"Data Science administrator groups\".</p> <p></p>"},{"location":"odh-rhoai/configuration/#advanced-configuration","title":"Advanced configuration","text":""},{"location":"odh-rhoai/configuration/#dashboard-configuration","title":"Dashboard configuration","text":"<p>RHOAI or ODH main configuration is done through a Custom Resource (CR) of type <code>odhdashboardconfigs.opendatahub.io</code>.</p> <ul> <li>To get access to it, from your OpenShift console, navigate to Home-&gt;API Explorer, and filter for <code>OdhDashboardConfig</code>:</li> </ul> <p></p> <ul> <li>Click on <code>OdhDashboardConfig</code> and in the Instances tab, click on <code>odh-dashboard-config</code>:</li> </ul> <p></p> <ul> <li>You can now view and edit the YAML file to modify the configuration:</li> </ul> <p></p> <p>In the <code>spec</code> section, the following items are of interest:</p> <ul> <li><code>dashboardConfig</code>: The different toggles will allow you to activate/deactivate certain features. For example, you may want to hide Model Serving for your users or prevent them from importing custom images.</li> <li><code>notebookSizes</code>: This is where you can fully customize the sizes of the notebooks. You can modify the resources and add or remove sizes from the default configuration as needed.</li> <li><code>modelServerSizes</code>: This setting operates on the same concept as the previous setting but for model servers.</li> <li><code>notebookController</code>: In this section you will find various settings related to the Workbenches and how they are launched.</li> <li>If your GPUs are not correctly detected, the dropdown allowing you to select how many GPUs you want to use for a workbench will not be displayed. To force it, you can create/modify the parameter <code>gpuSetting</code> under <code>notebookController</code>. This will force the dropdown to appear, with the maximum being the number you set for the parameter. Example:</li> </ul> <pre><code>notebookController:\n    enabled: true\n    gpuSetting: '4'\n    ...\n</code></pre>"},{"location":"odh-rhoai/configuration/#adding-a-custom-application","title":"Adding a custom application","text":"<p>Let's say you have installed another application in your cluster and want to make it available through the dashboard. That's easy! A tile is, in fact, represented by a custom resource (CR) of type <code>OdhApplication</code>.</p> <p>In this example, we will add a tile to access the MLFlow UI (see the MLFlow installation instructions to test it).</p> <ul> <li>The file mlflow-tile.yaml provides you with an example of how to create the tile.</li> <li>Edit this file to set the <code>route</code> (the name of the Route CR) and <code>routeNamespace</code> parameters to where the UI is accessible. In this example, it is <code>mlflow-server</code>(route name) and <code>mlflow</code> (server). Apply this file to create the resource.</li> <li>Wait 1-2 minutes for the change to take effect. Your tile is now available in the Explore view (bottom left):</li> </ul> <p></p> <ul> <li>However, it is not yet enabled. To enable this tile, click on it in the Explorer view, then click the \"Enable\" button at the top of the description. You can also create a ConfigMap from the file cm-mlflow-enable.yaml.</li> <li>Wait another 1-2 minutes, and your tile is now ready to use in the Enabled view:</li> </ul> <p></p>"},{"location":"odh-rhoai/custom-notebooks/","title":"Custom Notebooks","text":"<p>Custom notebook images are useful if you want to add libraries that you often use, or that you require at a specific version different than the one provided in the base images. It's also useful if you need to use OS packages or applications, which you cannot install on the fly in your running environment.</p>"},{"location":"odh-rhoai/custom-notebooks/#image-source-and-pre-built-images","title":"Image source and Pre-built images","text":"<p>In the opendatahub-io-contrib/workbench-images repository, you will find the source code as well as pre-built images for a lot of use cases. A few of the available images are:</p> <ul> <li>Base and CUDA-enabled images for different \"lines\" of OS: UBI8, UBI9, and Centos Stream 9.</li> <li>Jupyter images enhanced with:<ul> <li>specific libraries like OptaPy or Monai,</li> <li>with integrated applications like Spark,</li> <li>providing other IDEs like VSCode or RStudio</li> </ul> </li> <li>VSCode</li> <li>RStudio</li> </ul> <p>All those images are constantly and automatically updated and rebuilt for the latest patch and fixes, and new releases are available regularly to provide new versions of the libraries or the applications.</p>"},{"location":"odh-rhoai/custom-notebooks/#building-your-own-images","title":"Building your own images","text":"<p>In the repository above, you will find many examples from the source code to help you understand how to create your own image. Here are a few rules, tips and examples to help you.</p>"},{"location":"odh-rhoai/custom-notebooks/#rules","title":"Rules","text":"<ul> <li>On OpenShift, every containers in a standard namespace (unless you modify security) run with a user with a random user id (uid), and the group id (gid) 0. Therefore, all the folders that you want to write in, and all the files you want to modify (temporarily) in your image must be accessible by this user. The best practice is to set the ownership at 1001:0 (user \"default\", group \"0\").</li> <li>If you don't want/can't do that, another solution is to set permissions properly for any user, like 775.</li> <li>When launching a notebook from Applications-&gt;Enabled, the \"personal\" volume of a user is mounted at <code>/opt/app-root/src</code>. This is not configurable, so make sure to build your images with this default location for the data that you want persisted.</li> </ul>"},{"location":"odh-rhoai/custom-notebooks/#how-tos","title":"How-tos","text":""},{"location":"odh-rhoai/custom-notebooks/#install-python-packages","title":"Install Python packages","text":"<ul> <li>Start from a base image of your choice. Normally it's already running under user 1001, so no need to change it.</li> <li>Copy your pipfile.lock or your requirements.txt</li> <li>Install your packages</li> </ul> <p>Example:</p> <pre><code>FROM BASE_IMAGE\n\n# Copying custom packages\nCOPY Pipfile.lock ./\n\n# Install packages and cleanup\n# (all commands are chained to minimize layer size)\nRUN echo \"Installing softwares and packages\" &amp;&amp; \\\n    # Install Python packages \\\n    micropipenv install &amp;&amp; \\\n    rm -f ./Pipfile.lock\n    # Fix permissions to support pip in OpenShift environments \\\n    chmod -R g+w /opt/app-root/lib/python3.9/site-packages &amp;&amp; \\\n    fix-permissions /opt/app-root -P\n\nWORKDIR /opt/app-root/src\n\nENTRYPOINT [\"start-notebook.sh\"]\n</code></pre> <p>In this example, the fix-permissions script (present in all standard images and custom images from the opendatahub-contrib repo) fixes any bad ownership or rights that may be present.</p>"},{"location":"odh-rhoai/custom-notebooks/#install-an-os-package","title":"Install an OS package","text":"<ul> <li>If you have to install OS packages and Python packages, it's better to start with the OS.</li> <li>In your Containerfile/Dockerfile, switch to user 0, install your package(s), then switch back to user 1001. Example:</li> </ul> <pre><code>USER 0\n\nRUN INSTALL_PKGS=\"java-11-openjdk java-11-openjdk-devel\" &amp;&amp; \\\n    yum install -y --setopt=tsflags=nodocs $INSTALL_PKGS &amp;&amp; \\\n    yum -y clean all --enablerepo='*'\n\nUSER 1001\n</code></pre>"},{"location":"odh-rhoai/custom-notebooks/#tips-and-tricks","title":"Tips and tricks","text":""},{"location":"odh-rhoai/custom-notebooks/#enabling-codeready-builder-crb-and-epel","title":"Enabling CodeReady Builder (CRB) and EPEL","text":"<p>CRB and EPEL are repositories providing packages absent from a standard RHEL or UBI installation. They are useful and required to be able to install specific software (RStudio, I'm looking at you...).</p> <ul> <li>Enabling EPEL on UBI9-based images (on UBI9 images CRB is now enabled by default.):</li> </ul> <pre><code>RUN yum install -y https://download.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm\n</code></pre> <ul> <li>Enabling CRB and EPEL on Centos Stream 9-based images:</li> </ul> <pre><code>RUN yum install -y yum-utils &amp;&amp; \\\n    yum-config-manager --enable crb &amp;&amp; \\\n    yum install -y https://download.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm\n</code></pre>"},{"location":"odh-rhoai/custom-notebooks/#minimizing-image-size","title":"Minimizing image size","text":"<p>A container image uses a \"layered\" filesystem. Every time you have in your file a COPY or a RUN command, a new layer is created. Nothing is ever deleted: removing a file is simply \"masking\" it in the next layer. Therefore you must bee very careful when you create your Containerfile/Dockerfile.</p> <ul> <li>If you start from an image that is constantly updated, like ubi9/python-39 from the Red Hat Catalog, don't do a <code>yum update</code>. This will only fetch new metadata, update a few files that may not have any impact, and get you a bigger image.</li> <li>Rebuilt your images often from scratch, but don't do a <code>yum update</code> on a previous version.</li> <li>Group your <code>RUN</code> commands as much as you can, add <code>&amp;&amp; \\</code> at the end of each line to chain your commands.</li> <li>If you need to compile something for building an image, use the multi-stage builds approach. Build the library or application in an intermediate container image, then copy the result to your final image. Otherwise, all the build artefacts will persist in your image...</li> </ul>"},{"location":"odh-rhoai/custom-runtime-triton/","title":"Deploying and using a Custom Serving Runtime in ODH/RHOAI","text":"<p>Although these instructions were tested mostly using RHOAI (Red Hat OpenShift AI), they apply to ODH (Open Data Hub) as well.</p>"},{"location":"odh-rhoai/custom-runtime-triton/#before-you-start","title":"Before you start","text":"<p>This document will guide you through the broad steps necessary to deploy a custom Serving Runtime in order to serve a model using the Triton Runtime (NVIDIA Triton Inference Server).</p> <p>While RHOAI supports your ability to add your own runtime, it does not support the runtimes themselves. Therefore, it is up to you to configure, adjust and maintain your custom runtimes.</p> <p>This document expects a bit of familiarity with RHOAI.</p> <p>The sources used to create this document are mostly:</p> <ul> <li>https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes</li> <li>https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver</li> <li>Official Red Hat OpenShift AI Documentation</li> </ul>"},{"location":"odh-rhoai/custom-runtime-triton/#adding-the-custom-triton-runtime","title":"Adding the custom triton runtime","text":"<ol> <li>Log in to your OpenShift AI with a user who is part of the RHOAI admin group.<ol> <li>(by default, cluster-admins and dedicated admins are).</li> </ol> </li> <li> <p>Navigate to the Settings menu, then Serving Runtimes</p> <p></p> </li> <li> <p>Click on the Add Serving Runtime button:</p> <p></p> </li> <li> <p>From the drop down menu, select **Multi-model serving platform.  The option for REST will be selected automatically:</p> <p></p> </li> <li> <p>Click on Start from scratch and in the window that opens up, paste the following YAML:   <pre><code># Copyright 2021 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\napiVersion: serving.kserve.io/v1alpha1\n# kind: ClusterServingRuntime     ## changed by EG\nkind: ServingRuntime\nmetadata:\n  name: triton-23.05-20230804\n  labels:\n    name: triton-23.05-20230804\n  annotations:\n    maxLoadingConcurrency: \"2\"\n    openshift.io/display-name: \"Triton runtime 23.05 - added on 20230804 - with /dev/shm\"\nspec:\n  supportedModelFormats:\n    - name: keras\n      version: \"2\" # 2.6.0\n      autoSelect: true\n    - name: onnx\n      version: \"1\" # 1.5.3\n      autoSelect: true\n    - name: pytorch\n      version: \"1\" # 1.8.0a0+17f8c32\n      autoSelect: true\n    - name: tensorflow\n      version: \"1\" # 1.15.4\n      autoSelect: true\n    - name: tensorflow\n      version: \"2\" # 2.3.1\n      autoSelect: true\n    - name: tensorrt\n      version: \"7\" # 7.2.1\n      autoSelect: true\n\n  protocolVersions:\n    - grpc-v2\n  multiModel: true\n\n  grpcEndpoint: \"port:8085\"\n  grpcDataEndpoint: \"port:8001\"\n\n  volumes:\n    - name: shm\n      emptyDir:\n        medium: Memory\n        sizeLimit: 2Gi\n  containers:\n    - name: triton\n      # image: tritonserver-2:replace   ## changed by EG\n      image: nvcr.io/nvidia/tritonserver:23.05-py3\n      command: [/bin/sh]\n      args:\n        - -c\n        - 'mkdir -p /models/_triton_models;\n          chmod 777 /models/_triton_models;\n          exec tritonserver\n          \"--model-repository=/models/_triton_models\"\n          \"--model-control-mode=explicit\"\n          \"--strict-model-config=false\"\n          \"--strict-readiness=false\"\n          \"--allow-http=true\"\n          \"--allow-sagemaker=false\"\n          '\n      volumeMounts:\n        - name: shm\n          mountPath: /dev/shm\n      resources:\n        requests:\n          cpu: 500m\n          memory: 1Gi\n        limits:\n          cpu: \"5\"\n          memory: 1Gi\n      livenessProbe:\n        # the server is listening only on 127.0.0.1, so an httpGet probe sent\n        # from the kublet running on the node cannot connect to the server\n        # (not even with the Host header or host field)\n        # exec a curl call to have the request originate from localhost in the\n        # container\n        exec:\n          command:\n            - curl\n            - --fail\n            - --silent\n            - --show-error\n            - --max-time\n            - \"9\"\n            - http://localhost:8000/v2/health/live\n        initialDelaySeconds: 5\n        periodSeconds: 30\n        timeoutSeconds: 10\n  builtInAdapter:\n    serverType: triton\n    runtimeManagementPort: 8001\n    memBufferBytes: 134217728\n    modelLoadingTimeoutMillis: 90000\n</code></pre></p> </li> <li>You will likely want to update the name , as well as other parameters.</li> <li>Click Add</li> <li> <p>Confirm the new Runtime is in the list, and re-order the list as needed.   (the order chosen here is the order in which the users will see these choices)</p> <p></p> </li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#creating-a-project","title":"Creating a project","text":"<ul> <li>Create a new Data Science Project</li> <li>In this example, the project is called fraud</li> </ul>"},{"location":"odh-rhoai/custom-runtime-triton/#creating-a-model-server","title":"Creating a model server","text":"<ol> <li>In your project, scroll down to the \"Models and Model Servers\" Section</li> <li> <p>Click on Configure server</p> <p></p> </li> <li> <p>Fill out the details:</p> <p></p> <p></p> </li> <li> <p>Click Configure</p> </li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#deploying-a-model-into-it","title":"Deploying a model into it","text":"<ol> <li>If you don't have any model files handy, you can grab a copy of this file and upload it to your Object Storage of choice.</li> <li> <p>Click on Deploy Model</p> <p></p> </li> <li> <p>Choose a model name and a framework:</p> <p></p> </li> <li> <p>Then create a new data connection containing the details of where your model is stored in Object Storage:</p> <p></p> </li> <li> <p>After a little while, you should see the following:</p> <p></p> </li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#validating-the-model","title":"Validating the model","text":"<ol> <li>If you've used the model mentioned earlier in this document, you can run the following command from a Linux prompt:   <pre><code>function val-model {\n    myhost=\"$1\"\n    echo \"validating host $myhost\"\n    time curl -X POST -k \"${myhost}\" -d '{\"inputs\": [{ \"name\": \"dense_input\", \"shape\": [1, 7], \"datatype\": \"FP32\", \"data\": [57.87785658389723,0.3111400080477545,1.9459399775518593,1.0,1.0,0.0,0.0]}]}' | jq\n}\n\nval-model \"https://fraud-model-fraud.apps.mycluster.openshiftapps.com/v2/models/fraud-model/infer\"\n</code></pre></li> <li>Change the host to match the address for your model.</li> <li>You should see an output similar to:   <pre><code>{\n  \"model_name\": \"fraud-model__isvc-c1529f9667\",\n  \"model_version\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"dense_3\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        1,\n        1\n      ],\n      \"data\": [\n        0.86280495\n      ]\n    }\n  ]\n}\n</code></pre></li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#extra-considerations-for-disconnected-environments","title":"Extra considerations for Disconnected environments.","text":"<p>The YAML included in this file makes a reference to the following Nvidia Triton Image: <code>nvcr.io/nvidia/tritonserver:23.05-py3</code></p> <p>Ensure that this image is properly mirrored into the mirror registry.</p> <p>Also, update the YAML definition as needed to point to the image address that matches the image registry.</p>"},{"location":"odh-rhoai/custom-runtime-triton/#gitops-related-information","title":"GitOps related information","text":"<p>Each of the activities performed via the user interface will create a Kubernetes Object inside your OpenShift Cluster.</p> <ul> <li>The addition of a new runtime creates a <code>template</code> in the <code>redhat-ods-applications</code> namespace.</li> <li>Each model server is defined as a <code>ServingRuntime</code></li> <li>Each model is defined as an <code>InferenceService</code></li> <li>Each Data Connection is stored as a <code>Secret</code></li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/","title":"Working with GPUs","text":""},{"location":"odh-rhoai/nvidia-gpus/#using-nvidia-gpus-on-openshift","title":"Using NVIDIA GPUs on OpenShift","text":""},{"location":"odh-rhoai/nvidia-gpus/#how-does-this-work","title":"How does this work?","text":"<p>NVIDIA GPUs can be easily installed on OpenShift. Basically it involves installing two different operators.</p> <p>The Node Feature Discovery operator will \"discover\" your cards from a hardware perspective and appropriately label the relevant nodes with this information.</p> <p>Then the NVIDIA GPU operator will install the necessary drivers and tooling to those nodes. It will also integrate into Kubernetes so that when a Pod requires GPU resources it will be scheduled on the right node, and make sure that the containers are \"injected\" with the right drivers,  configurations and tools to properly use the GPU.</p> <p>So from a user perspective, the only thing you have to worry about is asking for GPU resources when defining your pods, with something like:</p> <pre><code>spec:\n  containers:\n  - name: app\n    image: ...\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n        nvidia.com/gpu: 2\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre> <p>But don't worry, OpenShift AI and Open Data Hub take care of this part for you when you launch notebooks, workbenches, model servers, or pipeline runtimes!</p>"},{"location":"odh-rhoai/nvidia-gpus/#installation","title":"Installation","text":"<p>Here is the documentation you can follow:</p> <ul> <li>OpenShift AI documentation</li> <li>NVIDIA documentation (more detailed)</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#advanced-configuration","title":"Advanced configuration","text":""},{"location":"odh-rhoai/nvidia-gpus/#working-with-taints","title":"Working with taints","text":"<p>In many cases, you will want to restrict access to GPUs, or be able to provide choice between different types of GPUs: simply stating \"I want a GPU\" is not enough. Also, if you want to make sure that only the Pods requiring GPUs end up on GPU-enabled nodes (and not other Pods that just end up being there at random because that's how Kubernetes works...), you're at the right place!</p> <p>The only supported method at the moment to achieve this is to taint nodes, then apply tolerations on the Pods depending on where you want them scheduled. If you don't pay close attention though when applying taints on Nodes, you may end up with the NVIDIA drivers not installed on those nodes...</p> <p>In this case you must:</p> <ul> <li> <p>Apply the taints you need to your Nodes or MachineSets, for example:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  ...\nspec:\n  replicas: 1\n  selector:\n    ...\n  template:\n    ...\n    spec:\n      ...\n      taints:\n        - key: restrictedaccess\n          value: \"yes\"\n          effect: NoSchedule\n</code></pre> </li> <li> <p>Apply the relevant toleration to the NVIDIA Operator.</p> <ul> <li> <p>In the <code>nvidia-gpu-operator</code> namespace, get to the Installed Operator menu, open the NVIDIA GPU Operator settings, get to the ClusterPolicy tab, and edit the ClusterPolicy.</p> <p></p> </li> <li> <p>Edit the YAML, and add the toleration in the daemonset section:</p> <pre><code>apiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  ...\n  name: gpu-cluster-policy\nspec:\n  vgpuDeviceManager: ...\n  migManager: ...\n  operator: ...\n  dcgm: ...\n  gfd: ...\n  dcgmExporter: ...\n  cdi: ...\n  driver: ...\n  devicePlugin: ...\n  mig: ...\n  sandboxDevicePlugin: ...\n  validator: ...\n  nodeStatusExporter: ...\n  daemonsets:\n    ...\n    tolerations:\n      - effect: NoSchedule\n        key: restrictedaccess\n        operator: Exists\n  sandboxWorkloads: ...\n  gds: ...\n  vgpuManager: ...\n  vfioManager: ...\n  toolkit: ...\n...\n</code></pre> </li> </ul> </li> </ul> <p>That's it, the operator is now able to deploy all the NVIDIA tooling on the nodes, even if they have the <code>restrictedaccess</code> taint. Repeat the procedure for any other taint you want to apply to your nodes.</p> <p>Note</p> <p>The first taint that you want to apply on GPU nodes is <code>nvidia.com/gpu</code>. This is the standard taint for which the NVIDIA Operator has a built-in toleration, so no need to add it. Likewise, Notebooks, Workbenches or other components from ODH/RHOAI that request GPUs will already have this toleration in place. For other Pods you schedule yourself, or using Pipelines, you should make sure the toleration is also applied. Doing this will ensure that only Pods really requiring GPUs are scheduled on those nodes.</p> <p>You can of course apply many different taints at the same time. You would simply have to apply the matching toleration on the NVIDIA GPU Operator, as well as on the Pods that need to run there.</p>"},{"location":"odh-rhoai/nvidia-gpus/#autoscaler-and-gpus","title":"Autoscaler and GPUs","text":"<p>As they are expensive, GPUs are good candidates to put behind an Autoscaler. But due to this there are some subtleties if you want everything to go smoothly.</p>"},{"location":"odh-rhoai/nvidia-gpus/#configuration","title":"Configuration","text":"<p>Warning</p> <p>For the autoscaler to work properly with GPUs, you have to set a specific label to the MachineSet. It will help to Autoscaler figure out (in fact simulate) what it is allowed to do. This is especially true if you have different MachineSets that feature different types of GPUs.</p> <p>As per the referenced article above, the <code>type</code> for gpus you set through the label cannot be <code>nvidia.com/gpu</code> (as you will sometimes find in the standard documentation), because it's not a valid label. Therefore, only for the autoscaling purpose, you should give the <code>type</code> a specific name with letters, numbers and dashes only, like <code>Tesla-T4-SHARED</code> in this example.</p> <ul> <li> <p>Edit the MachineSet configuration to add the label that the Autoscaler will expect:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      metadata:\n        labels:\n          cluster-api/accelerator: Tesla-T4-SHARED\n</code></pre> </li> <li> <p>Create your ClusterAutoscaler configuration (example):</p> <pre><code>apiVersion: autoscaling.openshift.io/v1\nkind: ClusterAutoscaler\nmetadata:\n  name: \"default\"\nspec:\n  logVerbosity: 4\n  maxNodeProvisionTime: 15m\n  podPriorityThreshold: -10\n  resourceLimits:\n    gpus:\n      - type: Tesla-T4-SHARED\n        min: 0\n        max: 8\n  scaleDown:\n    enabled: true\n    delayAfterAdd: 20m\n    delayAfterDelete: 5m\n    delayAfterFailure: 30s\n    unneededTime: 5m\n</code></pre> <p>Note</p> <p>The <code>delayAfterAdd</code> parameter has to be set higher than standard value as NVIDIA tooling can take a lot of time to deploy, 10-15mn.</p> </li> <li> <p>Create the MachineSet Autoscaler:</p> <pre><code>apiVersion: autoscaling.openshift.io/v1beta1\nkind: MachineAutoscaler\nmetadata:\n  name: machineset-name\n  namespace: \"openshift-machine-api\"\nspec:\n  minReplicas: 1\n  maxReplicas: 2\n  scaleTargetRef:\n    apiVersion: machine.openshift.io/v1beta1\n    kind: MachineSet\n    name: machineset-name\n</code></pre> </li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#scaling-to-zero","title":"Scaling to zero","text":"<p>As GPUs are expensive resources, you may want to scale down your MachineSet to zero to save on resources. This will however require some more configuration than just setting the minimum size to zero...</p> <p>First, some background to help you understand and enable you to solve issues that may arise. You can skip the whole explanation, but it's worth it, so please bear with me.</p> <p>When you request resources that aren't available, the Autoscaler looks at all the MachineAutoscalers that are available, with their corresponding MachineSets. But how to know which one to use? Well, it will first simulate the provisioning of a Node from each MachineSet, and see if it would fit the request. Of course, if there is already at least one Node available from a given MachineSet, the simulation would be bypassed as the Autoscaler already knows what it will get. If there are different MachineSets that fit and to choose from, the default and only \"Expander\" available for now in OpenShift to make its decision is <code>random</code>. So it will simply picks one totally randomly.</p> <p>That's all perfect and everything, but for GPUs, if you don't start the Node for real, we don't know what's in it! So that's where we have to help the Autoscaler with a small hint.</p> <ul> <li> <p>Set this annotation manually if it's not there. It will stick after the first scale up though, along with some other annotations the Autoscaler will add, thanks for its newly discovered knowledge.</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  annotations:\n    machine.openshift.io/GPU: \"1\"\n</code></pre> </li> </ul> <p>Now to the other issue that may happen if you are in an environment with multiple Availability Zones (AZ)...</p> <p>Although when you define a MachineSet you can set the AZ and have all the Nodes spawned properly in it, the Autoscaler simulator is not that clever. So it will simply pick a Zone at random. If this is not the one where you want/need your Pod to run, this will be a problem...</p> <p>For example, you may already have a Persistent Volume (PV) attached to you Notebook. If your storage does now support AZ-spanning (like AWS EBS volumes), your PV is bound to a specific AZ. If the Simulator creates a virtual Node in a different AZ, there will be a mismatch, your Pod would not be schedulable on this Node, and the Autoscaler will (wrongly) conclude that it cannot use this MachineSet for a scale up!</p> <p>Here again, we have to give a hint to the Autoscaler to what the Node will look like in the end.</p> <ul> <li> <p>In you MachineSet, in the labels that will be added to the node, add information regarding the topology of the Node, as well as for the volumes that may be attached to it. For example:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\nspec:\n  template:\n    spec:\n      metadata:\n        labels:\n          ...\n          topology.kubernetes.io/zone: us-east-2a\n          topology.ebs.csi.aws.com/zone: us-east-2a\n</code></pre> </li> </ul> <p>With this, the simulated Node will be at the right place, and the Autoscaler will consider the MachineSet valid for scale up!</p> <p>Reference material:</p> <ul> <li>https://cloud.redhat.com/blog/autoscaling-nvidia-gpus-on-red-hat-openshift</li> <li>https://access.redhat.com/solutions/6055181</li> <li>https://bugzilla.redhat.com/show_bug.cgi?id=1943194</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#gpu-partitioning-sharing","title":"GPU Partitioning / Sharing","text":"<p>There are also situations where the GPU(s) you have access to might be oversized for the task at hand, and having a single user or process lock-up and \"hog\" that GPU can be inefficient.  There are thankfully some partitioning strategies that can be brought to bear in order to deal with these situations.  Although there are multiple techniques, with various pros and cons, the net effect of these implementations is that what used to look like a single GPU will then look like multiple GPUs.  Obviously, there is no magic in the process, and the laws of physics still hold: there are trade-offs, and the multiple \"partitioned\" GPUs are not going to be faster or crunch more data than the real underlying physical GPU.  </p> <p>If this is a situation that you are facing, consult this repository for more detailed information and examples.</p>"},{"location":"odh-rhoai/nvidia-gpus/#time-slicing-gpu-sharing","title":"Time Slicing (GPU sharing)","text":"<p>Do you want to share GPUs between different Pods? Time Slicing is one of the solutions you can use!</p> <p>The NVIDIA GPU Operator enables oversubscription of GPUs through a set of extended options for the NVIDIA Kubernetes Device Plugin. GPU time-slicing enables workloads that are scheduled on oversubscribed GPUs to interleave with one another.</p> <p>This mechanism for enabling time-slicing of GPUs in Kubernetes enables a system administrator to define a set of replicas for a GPU, each of which can be handed out independently to a pod to run workloads on. Unlike Multi-Instance GPU (MIG), there is no memory or fault-isolation between replicas, but for some workloads this is better than not being able to share at all. Internally, GPU time-slicing is used to multiplex workloads from replicas of the same underlying GPU.</p> <ul> <li>Time Slicing Full reference</li> <li>Time Slicing Example Repository</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#configuration_1","title":"Configuration","text":"<p>This is a simple example on how to quickly setup Time Slicing on your OpenShift cluster. In this example, we have a MachineSet that can provide nodes with one T4 card each that we want to make \"seen\" as 4 different cards so that multiple Pods requiring GPUs can be launched, even if we only have one node of this type.</p> <ul> <li> <p>Create the ConfigMap that will define how we want to slice our GPU:</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: time-slicing-config\n  namespace: nvidia-gpu-operator\ndata:\n  tesla-t4: |-\n    version: v1\n    sharing:\n      timeSlicing:\n        resources:\n        - name: nvidia.com/gpu\n          replicas: 4\n</code></pre> <p>NOTE     - The ConfigMap has to be called <code>time-slicing-config</code> and must be created in the <code>nvidia-gpu-operator</code> namespace.     - You can add many different resources with different configurations. You simply have to provide the corresponding Node label that has been applied by the operator, for example <code>name: nvidia.com/mig-1g.5gb / replicas: 2</code> if you have a MIG configuration applied to a Node with a A100.     - You can modify the value of <code>replicas</code> to present less/more GPUs. Be warned though: all the Pods on this node will share the GPU memory, with no reservation. The more slices you create, the more risks of OOM errors (out of memory) you get if your Pods are hungry (or even only one!).</p> </li> <li> <p>Modify the ClusterPolicy called <code>gpu-cluster-policy</code> (accessible from the NVIDIA Operator view in the <code>nvidia-gpu-operator</code> namespace) to point to this configuration, and eventually add the default configuration (in case you nodes are not labelled correctly, see below)</p> <pre><code>apiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  ...\n  name: gpu-cluster-policy\nspec:\n  ...\n  devicePlugin:\n    config:\n      default: tesla-t4\n      name: time-slicing-config\n  ...\n</code></pre> </li> <li> <p>Apply label to your MachineSet for the specific slicing configuration you want to use on it:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\nspec:\n  template:\n    spec:\n      metadata:\n        labels:\n          nvidia.com/device-plugin.config: tesla-t4\n</code></pre> </li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#multi-instance-gpu-mig","title":"Multi-Instance GPU (MIG)","text":"<p>Multi-Instance GPU (MIG) enables a single physical GPU to be partitioned into several isolated instances, each with its own compute resources, memory, and performance profiles.</p> <p>There are two types of MIG strategies: <code>Single</code> and <code>Mixed</code>. The single MIG strategy should be utilized when all GPUs on a node have MIG enabled, while the <code>Mixed</code> MIG strategy should be utilized when not all GPUs on a node have MIG enabled.</p> <p>NOTE: MIG is only supported with the following NVIDIA GPU Types - A30, A100, A100X, A800, AX800, H100, H200, and H800.</p> <ul> <li>MIG Full reference</li> <li>MIG Single Example Repository</li> <li>MIG Mixed Example Repository</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#multi-process-service-mps","title":"Multi-Process Service (MPS)","text":"<p>Multi-Process Service (MPS) facilitates concurrent sharing of a single GPU among multiple CUDA applications.</p> <p>MPS is an alternative, binary-compatible implementation of the CUDA Application Programming Interface (API). The MPS runtime architecture is designed to transparently enable co-operative multi-process CUDA applications.</p> <ul> <li>MPS Full reference</li> <li>MPS Example Repository</li> </ul> <p>NOTE: Despite the tests passing, MPS isn't working correctly on OpenShift currently, due to only one process per GPU can run at any time. RH and NVIDIA engineers are working to fix this issue as soon as possible.</p>"},{"location":"odh-rhoai/nvidia-gpus/#aggregating-gpus-multi-gpu","title":"Aggregating GPUs (Multi-GPU)","text":"<p>Some Large Language Models (LLMs), such as Llama-3-70B and Falcon 180B, can be too large to fit into the memory of a single GPU (vRAM). Or in some cases, GPUs that would be large-enough might be difficult to obtain.  If you find yourself in such a situation, it is natural to wonder whether an aggregation of multiple, smaller GPUs can be used instead of one single large GPU.</p> <p>Thankfully, the answer is essentially Yes. To address these challenges, we can use more advanced configurations to distribute the LLM workload across several GPUs. One option is leveraging tensor parallelism, where the LLM is split across several GPUs, with each GPU processing a portion of the model's tensors. This approach ensures efficient utilization of available resources (GPUs) across one or several workers.</p> <p>Some Serving Runtimes, such as vLLM, support tensor parallelism, allowing for both single-worker and multi-worker configurations (the difference whether your GPUs are all in the same machine, or are spread across machines). </p> <p>vLLM has been added as an Out-of-the-box serving runtime, starting with Red Hat OpenShift AI version 2.10 link to our RHOAI doc</p> <p>For a detailed guide on implementing these solutions, refer to our repository.</p> <ul> <li>Single Worker Node - Multiple GPUs Example Repository</li> <li>Multiple Worker Node - Multiple GPUs Example Repository</li> </ul>"},{"location":"odh-rhoai/openshift-group-management/","title":"OpenShift Group Management","text":"<p>In the Red Hat OpenShift Documentation, there are instructions on how to configure a specific list of RHOAI Administrators and RHOAI Users.</p> <p>However, if the list of users keeps changing, the membership of the groupe called <code>rhods-users</code> will have to be updated frequently. By default, in OpenShift, only OpenShift admins can edit group membership. Being a RHOAI Admin does not confer you those admin privileges, and so, it would fall to the OpenShift admin to administer that list.</p> <p>The instructions in this page will show how the OpenShift Admin can create these groups in such a way that any member of the group <code>rhods-admins</code> can edit the users listed in the group <code>rhods-users</code>. These makes the RHOAI Admins more self-sufficient, without giving them unneeded access.</p> <p>For expediency in the instructions, we are using the <code>oc</code> cli, but these can also be achieved using the OpenShift Web Console. We will assume that the user setting this up has admin privileges to the cluster.</p>"},{"location":"odh-rhoai/openshift-group-management/#creating-the-groups","title":"Creating the groups","text":"<p>Here, we will create the groups mentioned above. Note that you can alter those names if you want, but will then need to have the same alterations throughout the instructions.</p> <ol> <li>To create the groups:     <pre><code>oc adm groups new rhods-users\noc adm groups new rhods-admins\n</code></pre></li> <li>The above may complain about the group(s) already existing.</li> <li>To confirm both groups exist:     <pre><code>oc get groups | grep rhods\n</code></pre></li> <li>That should return:     <pre><code>bash-4.4 ~ $ oc get groups | grep rhods\nrhods-admins\nrhods-users\n</code></pre></li> <li>Both groups now exist</li> </ol>"},{"location":"odh-rhoai/openshift-group-management/#creating-clusterrole-and-clusterrolebinding","title":"Creating ClusterRole and ClusterRoleBinding","text":"<ol> <li>This will create a Cluster Role and a Cluster Role Binding:     <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: update-rhods-users\nrules:\n  - apiGroups: [\"user.openshift.io\"]\n    resources: [\"groups\"]\n    resourceNames: [\"rhods-users\"]\n    verbs: [\"update\", \"patch\", \"get\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: rhods-admin-can-update-rhods-users\nsubjects:\n  - kind: Group\n    apiGroup: rbac.authorization.k8s.io\n    name: rhods-admins\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: update-rhods-users\nEOF\n</code></pre></li> <li>To confirm they were both succesfully created, run:     <pre><code>oc get ClusterRole,ClusterRoleBinding  | grep 'update\\-rhods'\n</code></pre></li> <li>You should see:     <pre><code>bash-4.4 ~ $ oc get ClusterRole,ClusterRoleBinding  | grep 'update\\-rhods'\nclusterrole.rbac.authorization.k8s.io/update-rhods-users\nclusterrolebinding.rbac.authorization.k8s.io/rhods-admin-can-update-rhods-users\n</code></pre></li> <li>You are pretty much done. You now just need to validate things worked.</li> </ol>"},{"location":"odh-rhoai/openshift-group-management/#add-some-users-as-rhods-admins","title":"Add some users as <code>rhods-admins</code>","text":"<p>To confirm this works, add a user to the <code>rhods-admin</code> group. In my example, I'll add <code>user1</code></p>"},{"location":"odh-rhoai/openshift-group-management/#capture-the-url-needed-to-edit-the-rhods-users-group","title":"Capture the URL needed to edit the <code>rhods-users</code> group","text":"<p>Since people who are not cluster admin won't be able to browse the list of groups, capture the URL that allows to control the membership of <code>rhods-users</code>.</p> <p>It should look similar to:</p> <p><code>https://console-openshift-console.apps.&lt;thecluster&gt;/k8s/cluster/user.openshift.io~v1~Group/rhods-users</code></p>"},{"location":"odh-rhoai/openshift-group-management/#ensure-that-rhods-admins-are-now-able-to-edit-rhods-users","title":"Ensure that <code>rhods-admins</code> are now able to edit <code>rhods-users</code>","text":"<p>Ask someone in the <code>rhods-admins</code> group to confirm that it works for them. (Remember to provide them with the URL to do so).</p> <p>They should be able to do so and successfully save their changes, as shown below:</p> <p></p>"},{"location":"odh-rhoai/single-stack-serving-certificate/","title":"Use Existing OpenShift Certificate for Single Stack Serving","text":"<p>By default, the Single Stack Serving in Openshift AI uses a self-signed certificate generated at installation for the endpoints that are created when deploying a server. This can be counter-intuitive because if you already have certificates configured on your OpenShift cluster, they will be used by default for other types of endpoints like Routes.</p> <p>The installation procedure for the Single Stack Serving available here (section 4.vi) states that you can provide your own certificate instead of using a self-signed one.</p> <p>This following procedure explains how to use the same certificate that you already have for your OpenShift cluster.</p>"},{"location":"odh-rhoai/single-stack-serving-certificate/#procedure","title":"Procedure","text":"<ul> <li>Configure OpenShift to use a valid certificate for accessing the Console and in general for any created Route (normally, this is something that was already done).</li> <li>From the openshift-ingress namespace, copy the content of a Secret whose name includes \"certs\". For example <code>rhods-internal-primary-cert-bundle-secret</code> or <code>ingress-certs-....</code>. The content of the Secret (data) should contain two items, tls.cert and tls.key. They are the certificate and key that are used for all the OpenShift Routes.</li> <li> <p>Cleanup the YAML to just keep the relevant content. It should look like this (the name of the secret will be different, it's normally tied to your cluster name):</p> <pre><code>kind: Secret\napiVersion: v1\nmetadata:\nname: rhods-internal-primary-cert-bundle-secret\ndata:\ntls.crt: &gt;-\n    LS0tLS1CRUd...\ntls.key: &gt;-\n    LS0tLS1CRUd...\ntype: kubernetes.io/tls\n</code></pre> </li> <li> <p>Apply this YAML into the istio-system namespace (basically, copy the Secret from one namespace to the other).</p> </li> <li> <p>Following the Single Stack Serving installation procedure, in your DSC Configuration, refer to this secret for the kserve configuration (don\u2019t forget to change the secretName for the one you just created):</p> <pre><code>kserve:\ndevFlags: {}\nmanagementState: Managed\nserving:\n    ingressGateway:\n    certificate:\n        secretName: rhods-internal-primary-cert-bundle-secret\n        type: Provided\n    managementState: Managed\n    name: knative-serving\n</code></pre> </li> </ul> <p>Your Model Servers will now be deployed with the same certificate as you are using for OpenShift Routes. If this is a trusted certificate, your Endpoints will be accessible using SSL without having to ignore error messages or create special configurations.</p>"},{"location":"odh-rhoai/single-stack-serving-certificate/#other-workarounds","title":"Other Workarounds","text":"<p>If the above method does not work or you don't want or can't do any modification, you can try to bypass the certificate verification in your application or code. Depending on the library used, there are different solutions.</p>"},{"location":"odh-rhoai/single-stack-serving-certificate/#using-langchain-with-openai-api-compatible-runtimes","title":"Using Langchain with OpenAI API compatible runtimes","text":"<p>The underlying library used for communication by the base OpenAI module of Langchain is <code>httpx</code>. You can directly specify <code>httpx</code> settings when you instantiate the llm object in Langchain. With the following settings on the last two lines of this example, any certificate error will be ignored:</p> <pre><code>import httpx\n# LLM definition\nllm = VLLMOpenAI(\n    openai_api_key=\"EMPTY\",\n    openai_api_base= f\"{inference_server_url}/v1\",\n    model_name=\"/mnt/models/\",\n    top_p=0.92,\n    temperature=0.01,\n    max_tokens=512,\n    presence_penalty=1.03,\n    streaming=True,\n    callbacks=[StreamingStdOutCallbackHandler()]\n    async_client=httpx.AsyncClient(verify=False),\n    http_client=httpx.Client(verify=False)\n)\n</code></pre>"},{"location":"odh-rhoai/from-zero-to-workbench/using-cli/","title":"From Zero To Workbench using the CLI","text":"<p>In this repo, you will find all the straightforward instructions to quickly deploy OpenShift AI only using a CLI or automation.</p> <p>Detailed instructions, YAMLs, code, all is there to get you quickly started!</p> <p>Note</p> <p>This documentation was produced working off of mainly OpenShift v4.14 and OpenShift AI 2.7. The artifacts directory contains sample configuration files which are used throughout this repo to demonstrate various concepts. While an effort will be made to ensure that these artifacts stay up to date, there is a possibility that they will not always work as intended.</p> <p>Suggestions and pull requests are welcome to maintain this content up to date!</p>"},{"location":"odh-rhoai/from-zero-to-workbench/using-developer-hub/","title":"OpenShift AI on Developer Hub","text":"<p>In this repo, you will find a Backstage Golden Path Template for a Red Hat OpenShift AI Data science project. Detailed instructions, examples, all is there to get you quickly started!</p> <p>Note</p> <p>This documentation is a work in progress that provides recipes for some components only.</p> <p>Suggestions and pull requests are welcome to maintain this content up to date and make it evolve!</p>"},{"location":"odh-rhoai/from-zero-to-workbench/using-ui/","title":"From Zero To Workbench using the GUI","text":"<p>In this repo, you will find all the straightforward instructions to quickly deploy OpenShift AI using mainly the OpenShift Console UI.</p> <p>Detailed instructions, screenshots, examples, all is there to get you quickly started!</p> <p>Note</p> <p>This documentation was produced working off of mainly OpenShift v4.14 and OpenShift AI 2.7. The artifacts directory contains sample configuration files which are used throughout this repo to demonstrate various concepts. While an effort will be made to ensure that these artifacts stay up to date, there is a possibility that they will not always work as intended.</p> <p>Suggestions and pull requests are welcome to maintain this content up to date!</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/","title":"Bucket Notifications","text":""},{"location":"patterns/bucket-notifications/bucket-notifications/#description","title":"Description","text":"<p>The Rados Gateway (RGW) component of Ceph provides Object Storage through an S3-compatible API on all Ceph implementations: OpenShift Data Foundation and its upstream version Rook-Ceph, Red Hat Ceph Storage, Ceph,\u2026\u200b</p> <p>Bucket notifications provide a mechanism for sending information from the RGW when certain events are happening on a bucket. Currently, notifications can be sent to: HTTP, AMQP0.9.1 and Kafka endpoints.</p> <p>From a data engineering point of view, bucket notifications allow to create an event-driven architecture, where messages (instead of simply log entries) can be sent to various processing components or event buses whenever something is happening on the object storage: object creation, deletion, with many fine-grained settings available.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#use-cases","title":"Use cases","text":""},{"location":"patterns/bucket-notifications/bucket-notifications/#application-taking-actions-on-the-objects","title":"Application taking actions on the objects","text":"<p>As part of an event-driven architecture, this pattern can be used to trigger an application to perform an action following the storage event. An example could be the automated processing of a new image that has just been uploaded to the object storage (analysis, resizing,\u2026\u200b). Paired with Serverless functions this becomes a pretty efficient architecture compared to having an application constantly monitoring or polling the storage, or to have to implement this triggering process in the application interacting with the storage. This loosely-coupled architecture also gives much more agility for updates, technology evolution,\u2026\u200b</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#external-monitoring-systems","title":"External monitoring systems","text":"<p>The events sent by the RGW are simple messages containing all the metadata relevant to the event and the object. So it can be an excellent source of information for a monitoring system. For example if you want to keep a trace or send an alert whenever a specific type of file, or with a specific name, is uploaded or deleted from the storage.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#implementations-examples","title":"Implementations examples","text":"<p>This pattern is implemented in the XRay pipeline demo</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#how-does-it-work","title":"How does it work?","text":""},{"location":"patterns/bucket-notifications/bucket-notifications/#characteristics","title":"Characteristics","text":"<ul> <li>Notifications are sent directly from the RGW on which the event happened to an external endpoint.</li> <li>Pluggable endpoint architecture:<ul> <li>HTTP/S</li> <li>AMQP 0.9.1</li> <li>Kafka</li> <li>Knative</li> </ul> </li> </ul>"},{"location":"patterns/bucket-notifications/bucket-notifications/#data-model","title":"Data Model","text":"<ul> <li>Topics contain the definition of a specific endpoint in \u201cpush mode\u201d</li> <li>Notifications tie topics with buckets, and may also include filter definition on the events</li> </ul>"},{"location":"patterns/bucket-notifications/bucket-notifications/#configuration","title":"Configuration","text":"<p>This configuration shows how to create a notification that will send a message (event) to a Kafka topic when a new object is created in a bucket.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#requirements","title":"Requirements","text":"<ul> <li>Access to a Ceph/ODF/RHCS installation with the RGW deployed.</li> <li>Endpoint address (URL) for the RGW.</li> <li>Credentials to connect to the RGW:<ul> <li>AWS_ACCESS_KEY_ID</li> <li>AWS_SECRET_ACCESS_KEY</li> </ul> </li> </ul> <p>Note</p> <p>As Ceph implements an S3-Compatible API to access Object Storage, standard naming for variables or procedures used with S3 were retained to stay coherent with examples, demos or documentation related to S3. Therefore the AWS prefix in the previous variables.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#topic-creation","title":"Topic Creation","text":"<p>A topic is the definition of a specific endpoint. It must be created first.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-1-raw-configuration","title":"Method 1: \"RAW\" configuration","text":"<p>As everything is done through the RGW API, you can query it directly. To be fair, this method is almost never used (unless there is no SDK or S3 tool for your environment) but gives a good understanding of the process.</p> <p>Example for a Kafka Endpoint:</p> <pre><code>POST\nAction=CreateTopic\n&amp;Name=my-topic\n&amp;push-endpoint=kafka://my-kafka-broker.my-net:9999\n&amp;Attributes.entry.1.key=verify-ssl\n&amp;Attributes.entry.1.value=true\n&amp;Attributes.entry.2.key=kafka-ack-level\n&amp;Attributes.entry.2.value=broker\n&amp;Attributes.entry.3.key=use-ssl\n&amp;Attributes.entry.3.value=true\n&amp;Attributes.entry.4.key=OpaqueData\n&amp;Attributes.entry.4.value=https://s3-proxy.my-zone.my-net\n</code></pre> <p>Note</p> <p>The authentication part is not detailed here as the mechanism is pretty convoluted, but it is directly implemented in most API development tools, like Postman.</p> <p>The full reference for the REST API for bucket notifications is available here.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-2-python-aws-sdk","title":"Method 2: Python + AWS SDK","text":"<p>As the creator of the S3 API, AWS is providing SDKs for the main languages to interact with it. Thanks to this compatibility, you can use those SDKs to interact with Ceph in the same way. For Python, the library to interact with AWS services is called boto3.</p> <p>Example for a Kafka Endpoint:</p> <pre><code>import boto3\nsns = boto3.client('sns',\n                endpoint_url = endpoint_url,\n                aws_access_key_id = aws_access_key_id,\n                aws_secret_access_key= aws_secret_access_key,\n                region_name='default',\n                config=botocore.client.Config(signature_version = 's3'))\n\nattributes = {}\nattributes['push-endpoint'] = 'kafka://my-cluster-kafka-bootstrap:9092'\nattributes['kafka-ack-level'] = 'broker'\n\ntopic_arn = sns.create_topic(Name=my-topic, Attributes=attributes)['TopicArn']\n</code></pre>"},{"location":"patterns/bucket-notifications/bucket-notifications/#notification-configuration","title":"Notification Configuration","text":"<p>The notification configuration will \"tie\" a bucket with a topic.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-1-raw-configuration_1","title":"Method 1: \"RAW\" configuration","text":"<p>As previously, you can directly query the RGW REST API. This is done through an XML formatted payload that is sent with a PUT command.</p> <p>Example for a Kafka Endpoint:</p> <pre><code>PUT /my-bucket?notification HTTP/1.1\n\n&lt;NotificationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;\n    &lt;TopicConfiguration&gt;\n        &lt;Id&gt;my-notification&lt;/Id&gt;\n        &lt;Topic&gt;my-topic&lt;/Topic&gt;\n        &lt;Event&gt;s3:ObjectCreated:*&lt;/Event&gt;\n        &lt;Event&gt;s3:ObjectRemoved:DeleteMarkerCreated&lt;/Event&gt;\n    &lt;/TopicConfiguration&gt;\n    &lt;TopicConfiguration&gt;\n...\n    &lt;/TopicConfiguration&gt;\n&lt;/NotificationConfiguration&gt;\n</code></pre> <p>Again, the full reference for the REST API for bucket notifications is available here.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-2-python-aws-sdk_1","title":"Method 2: Python + AWS SDK","text":"<p>Example for a Kafka Endpoint:</p> <pre><code>import boto3\ns3 = boto3.client('s3',\n                endpoint_url = endpoint_url,\n                aws_access_key_id = aws_access_key_id,\n                aws_secret_access_key = aws_secret_access_key,\n                region_name = 'default',\n                config=botocore.client.Config(signature_version = 's3'))\n\nbucket_notifications_configuration = {\n            \"TopicConfigurations\": [\n                {\n                    \"Id\": 'my-id',\n                    \"TopicArn\": 'arn:aws:sns:s3a::my-topic',\n                    \"Events\": [\"s3:ObjectCreated:*\"]\n                }\n            ]\n        }\n\ns3.put_bucket_notification_configuration(Bucket = bucket_name,\n        NotificationConfiguration=bucket_notifications_configuration)\n</code></pre>"},{"location":"patterns/bucket-notifications/bucket-notifications/#filters","title":"Filters","text":"<p>Although a notification is specific to a bucket (and you can have multiple configurations on one bucket), you may want that it does not apply to all the objects from this bucket. For example you want to send an event when an image is uploaded, but not do anything it\u2019s another type of file. You can do this with filters! And not only on the filename, but also on the tags associated to it in its metadata.</p> <p>Filter examples, on keys or tags:</p> <pre><code>&lt;Filter&gt;\n    &lt;S3Key&gt;\n        &lt;FilterRule&gt;\n         &lt;Name&gt;regex&lt;/Name&gt;\n         &lt;Value&gt;([0-9a-zA-Z\\._-]+.(png|gif|jp[e]?g)&lt;/Value&gt;\n        &lt;/FilterRule&gt;\n    &lt;/S3Key&gt;\n    &lt;S3Tags&gt;\n        &lt;FilterRule&gt;\n            &lt;Name&gt;Project&lt;/Name&gt;&lt;Value&gt;Blue&lt;/Value&gt;\n        &lt;/FilterRule&gt;\n        &lt;FilterRule&gt;\n            &lt;Name&gt;Classification&lt;/Name&gt;&lt;Value&gt;Confidential&lt;/Value&gt;\n        &lt;/FilterRule&gt;\n    &lt;/S3Tags&gt;\n&lt;/Filter&gt;\n</code></pre>"},{"location":"patterns/bucket-notifications/bucket-notifications/#events","title":"Events","text":"<p>The notifications sent to the endpoints are called events, and they are structured like this:</p> <p>Event example:</p> <pre><code>{\"Records\":[\n    {\n        \"eventVersion\":\"2.1\",\n        \"eventSource\":\"ceph:s3\",\n        \"awsRegion\":\"us-east-1\",\n        \"eventTime\":\"2019-11-22T13:47:35.124724Z\",\n        \"eventName\":\"ObjectCreated:Put\",\n        \"userIdentity\":{\n            \"principalId\":\"tester\"\n        },\n        \"requestParameters\":{\n            \"sourceIPAddress\":\"\"\n        },\n        \"responseElements\":{\n            \"x-amz-request-id\":\"503a4c37-85eb-47cd-8681-2817e80b4281.5330.903595\",\n            \"x-amz-id-2\":\"14d2-zone1-zonegroup1\"\n        },\n        \"s3\":{\n            \"s3SchemaVersion\":\"1.0\",\n            \"configurationId\":\"mynotif1\",\n            \"bucket\":{\n                \"name\":\"mybucket1\",\n                \"ownerIdentity\":{\n                    \"principalId\":\"tester\"\n                },\n                \"arn\":\"arn:aws:s3:us-east-1::mybucket1\",\n                \"id\":\"503a4c37-85eb-47cd-8681-2817e80b4281.5332.38\"\n            },\n            \"object\":{\n                \"key\":\"myimage1.jpg\",\n                \"size\":\"1024\",\n                \"eTag\":\"37b51d194a7513e45b56f6524f2d51f2\",\n                \"versionId\":\"\",\n                \"sequencer\": \"F7E6D75DC742D108\",\n                \"metadata\":[],\n                \"tags\":[]\n            }\n        },\n        \"eventId\":\"\",\n        \"opaqueData\":\"me@example.com\"\n    }\n]}\n</code></pre>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/","title":"Kafka to Object Storage","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#description","title":"Description","text":"<p>Kafka is a distributed event stream processing system which is great for storing hot relevant data. Based on the retention policy of the data, it can be used to store data for a long time. However, it is not suitable for storing data for a long time. This is where we need a mechanism to move data from Kafka to the object storage.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#use-cases","title":"Use Cases","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#long-term-retention-of-data","title":"Long term retention of data","text":"<p>As Kafka is not really suited for long term retention of data, persisting it inside an object store will allow you to keep your data for further use, backup or archival purposes. Depending on the solution you use, you can also transform or format you data while storing it, which will ease further retrieval.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#move-data-to-central-data-lake","title":"Move data to Central Data Lake","text":"<p>Production Kafka environment may not be the best place to run analytics or do model training. Transferring or copying the date to a central data lake will allow you to decouple those two aspects (production and analytics), bringing peace of mind and further capabilities to the data consumers.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#implementations-examples","title":"Implementations examples","text":"<p>This pattern is implemented in the Smart City demo</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#configuration-using-secor","title":"Configuration Using Secor","text":"<p>This pattern implements the Secor Kafka Consumer. It can be used to consume kafka messages from a kafka topic and store that to S3 compatible Objet Buckets.</p> <p>Secor is a service persisting Kafka logs to Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage and Openstack Swift. Its key features are: strong consistency, fault tolerance, load distribution, horizontal scalability, output partitioning, configurable upload policies, monitoring, customizability, event transformation.</p> <p></p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#prerequisites","title":"Prerequisites","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#bucket","title":"Bucket","text":"<p>An S3-compatible bucket, with its access key and secret key.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#zookeeper-entrance","title":"ZooKeeper Entrance","text":"<p>Secor needs to connect directly to Zookeeper to keep track of some data. If you have a secured installation of Zookeeper, like when you deploy Kafka using Strimzi or AMQStreams, you need to deploy a ZooKeeper Entrance. This is a special proxy to Zookeeper that will allow this direct connection.</p> <p>Note</p> <p>The deployment file is based on a Strimzi or AMQ Streams deployment of Kafka. If you configuration is different you may have to adapt some of the parameters.</p> <p>Deployment:</p> <ul> <li>In the file deployment/zookeeper-entrance.yaml, replace:<ul> <li>the occurrences of 'NAMESPACE' by the namespace where the Kafka cluster is.</li> <li>the occurrences of 'YOUR_KAFKA' by the name of your Kafka cluster.</li> <li>the parameters YOUR_KEY, YOUR_SECRET, YOUR_ENDPOINT, YOUR_BUCKET with the values corresponding to the bucket where you want to store the data.</li> </ul> </li> <li>Apply the modified file to deploy ZooKeeper Entrance.</li> </ul>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#deployment","title":"Deployment","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#secor","title":"Secor","text":"<ul> <li>In the file deployment/secor.yaml, replace:<ul> <li>the occurrences of 'NAMESPACE' by the namespace where the Kafka cluster is.</li> <li>the occurrences of 'YOUR_KAFKA' by the name of your Kafka cluster.</li> <li>adjust all the other Secor parameters or add others depending on the processing you want to do with the data: output format, aggregation,... Full instructions are available here.</li> </ul> </li> <li>Apply the modified file to deploy Secor.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/","title":"Kafka to Serverless","text":""},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#description","title":"Description","text":"<p>This pattern describes how to use AMQ Streams (Kafka) as an event source to OpenShift Serverless (Knative). You will learn how to implement Knative Eventing that can trigger a Knative Serving function when a messaged is posted to a Kafka Topic (Event).</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#knative-openshift-serverless","title":"Knative &amp; OpenShift Serverless","text":"<p>Knative is an open source project that helps to deploy and manage modern serverless workloads on Kubernetes. Red Hat OpenShift Serverless is an enterprise-grade serverless offering based on knative that provides developers with a complete set of tools to build, deploy, and manage serverless applications on OpenShift Container Platform</p> <p>Knative consists of 3 primary components:</p> <ul> <li>Build - A flexible approach to building source code into containers.</li> <li>Serving - Enables rapid deployment and automatic scaling of containers through a request-driven model for serving workloads based on demand.</li> <li>Eventing - An infrastructure for consuming and producing events to stimulate applications. Applications can be triggered by a variety of sources, such as events from your own applications, cloud services from multiple providers, Software-as-a-Service (SaaS) systems, and Red Hat AMQ streams.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#eda-event-driven-architecture","title":"EDA (Event Driven Architecture)","text":"<p>Event-Driven Architecture (EDA) is a way of designing applications and services to respond to real-time information based on the sending and receiving of information about individual events. EDA uses events to trigger and communicate between decoupled services and is common in modern applications built with microservices.</p> <p></p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#use-cases","title":"Use Cases","text":"<ul> <li>Develop an event-driven architecture with serverless applications.</li> <li>Serverless Business logic processing that is capable of automated scale-up and scale-down to zero.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#implementations-examples","title":"Implementations examples","text":"<p>This pattern is implemented in the XRay Pipeline Demo</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#deployment-example","title":"Deployment example","text":""},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#requirements","title":"Requirements","text":"<ul> <li>Red Hat OpenShift Container Platform</li> <li>Red Hat AMQ Streams or Strimzi: the operator should be installed and a Kafka cluster must be created</li> <li>Red Hat OpenShift Serverless: the operator must be installed</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-1-set-up-knative","title":"Part 1: Set up KNative","text":"<p>Once Red Hat OpenShift Serverless operator has been installed, we can create KnativeServing, KnativeEventing and KnativeKafka instances.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-1-create-required-knative-instances","title":"Step 1: Create required Knative instances","text":"<ul> <li>From the deployment folder, apply the YAML file 01_knative_serving_eventing_kafka_setup.yaml to create knative instances</li> </ul> <pre><code>oc create -f 01_knative_serving_eventing_kafka_setup.yaml\n</code></pre> <p>Note</p> <p>Those instances can also be deployed through the OpenShift Console if you prefer to use a UI. In this case, follow the Serverless deployment instructions (this section and the following ones).</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-2-verify-knative-instances","title":"Step 2: Verify Knative Instances","text":"<pre><code>oc get po -n knative-serving\noc get po -n knative-eventing\n</code></pre> <ul> <li>Pod with prefix kafka-controller-manager represents Knative Kafka Event Source.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-2-knative-serving","title":"Part 2: Knative Serving","text":"<p>Knative Serving is your serverless business logic that you would like to execute based on the event generated by Kafka.</p> <p>For example purpose we are using a simple greeter service here. Depending on your use case you will replace that with your own business logic.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-1-create-knative-serving","title":"Step 1: Create Knative Serving","text":"<ul> <li>From the deployment folder, in the YAML file 02_knative_service.yaml, replace the placeholder <code>YOUR_NAMESPACE</code> with your namespace, and apply the file to create knative serving.</li> </ul> <pre><code>oc create -f 02_knative_service.yaml\n</code></pre>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-2-verify-knative-serving","title":"Step 2: Verify Knative Serving","text":"<pre><code>oc get serving\n</code></pre>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-3-knative-eventing","title":"Part 3: Knative Eventing","text":"<p>Knative Eventing enables developers to use an event-driven architecture with serverless applications. An event-driven architecture is based on the concept of decoupled relationships between event producers that create events, and event sinks, or consumers, that receive them.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-1-kafka-topic","title":"Step 1: Kafka topic","text":"<ul> <li>Create a Kafka topic where the events will be sent. In this example, the topic will be <code>example_topic</code>.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-2-create-knative-eventing","title":"Step 2: Create Knative Eventing","text":"<ul> <li>To create a Knative Eventing, we need to create a Kafka Event Source. Before you apply the following YAML file, 03_knative_kafka_source.yaml, please make sure to edit namespace and bootstrapServers to match your Kafka cluster. Also make sure to use the correct Knative Service (serving) that you have created in the previous step (<code>greeter</code> in this example).</li> </ul> <pre><code>oc create -f 03_knative_kafka_source.yaml\n</code></pre>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-3-verify-knative-eventing","title":"Step 3: Verify Knative Eventing","text":"<pre><code>oc get kafkasource\n</code></pre> <p>At this point, as soon as new messages are received in Kafka topic <code>example_topic</code>, Knative Eventing will trigger the Knative Service greeter to execute the business logic, allowing you to have event-driven serverless application running on OpenShift Container Platform.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-4-testing","title":"Part 4: Testing","text":"<ul> <li>Optional: to view the logs of Knative Serving you can install stern to them from the CLI, or use the OpenShift Web Console.</li> </ul> <pre><code>oc get ksvc\nstern --selector=serving.knative.dev/service=greeter -c user-container\n</code></pre> <ul> <li>Launch a temporary Kafka CLI (kafkacat) in a new terminal</li> </ul> <pre><code>oc run kafkacat -i -t --image debezium/tooling --restart=Never\n</code></pre> <ul> <li>From the kafkacat container shell, generate kafka messages in the topic <code>example_topic</code> of your Kafka cluster. Here we are generating Kafka messages with CloudEvents (CE) specification.</li> </ul> <pre><code>for i in {1..50} ; do sleep 10 ; \\\necho '{\"message\":\"Hello Red Hat\"}' | kafkacat -P -b core-kafka-kafka-bootstrap -t example_topic \\\n  -H \"content-type=application/json\" \\\n  -H \"ce-id=CE-001\" \\\n  -H \"ce-source=/kafkamessage\"\\\n  -H \"ce-type=dev.knative.kafka.event\" \\\n  -H \"ce-specversion=1.0\" \\\n  -H \"ce-time=2018-04-05T03:56:24Z\"\ndone ;\n</code></pre> <p>The above command will generate 50 Kafka messages every 10 seconds. Knative Eventing will pick up the messages and invoke the greeter Knative service, that you can verify from the logs of Knative Serving.</p>"},{"location":"patterns/starproxy/starproxy/","title":"Starburst/Trino Proxy","text":""},{"location":"patterns/starproxy/starproxy/#what-it-is","title":"What it is","text":"<p>Starproxy is a fully HTTP compliant proxy that is designed to sit between clients and a Trino/Starburst cluster. The motivation for developing a solution like this is laid out in some prior art below:</p> <ul> <li>Facebook Engineering Blog - Static Analysis</li> <li>Strata Conference Talk</li> <li>Uber Case Study - Prism</li> </ul> <p>The most attractive items to us are probably:</p> <ul> <li>Enabling host based security</li> <li>Detecting \"bad\" queries and blocking/deprioritizing them with custom rules</li> <li>Load balancing across regions</li> </ul>"},{"location":"patterns/starproxy/starproxy/#how-it-works","title":"How it works","text":"<p>First and foremost, starproxy is an http proxy implemented in rust using a combination of axum/hyper.</p> <p></p> <ol> <li> <p>Parse the query AST, then check a variety of rules:</p> <ul> <li>inbound CIDR rule checking</li> <li>checking for predicates in queries</li> <li>identifying select * queries with no limit, among other rules</li> </ul> </li> <li> <p>If rules are violated they can be associated with actions, like tagging the query as low priority. This is done by modifying the request headers and injecting special tags. Rules can also outright block requests by returning error status codes to the client directly.</p> </li> </ol>"},{"location":"predictive-ai/what-is-predictive-ai/","title":"What is Predictive AI?","text":"<p>Predictive AI generally has the following characteristics:</p> <ul> <li>A model generally created based on specific or narrow set of data.</li> <li>Aims to make predictions on the likehood of an outcome, based on certain conditions and inputs.</li> <li>e.g. \"how likely is this person to default on their loan in the next 5 years, given what we know about them?\"</li> </ul>"},{"location":"tools-and-applications/airflow/airflow/","title":"Apache Airflow","text":""},{"location":"tools-and-applications/airflow/airflow/#what-is-it","title":"What is it?","text":"<p>Apache Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. It has become popular because of how easy it is to use and how extendable it is, covering a wide variety of tasks and allowing you to connect your workflows with virtually any technology. Since it's a Python framework it has also gathered a lot of interest from the Data Science field.</p> <p>One important concept used in Airflow is DAGs (Directed Acyclical Graphs). A DAG is a graph without any cycles. In other words, a node in your graph may never point back to a node higher up in your workflow. DAGs are used to model your workflows/pipelines, which essentially means that you are building and executing graphs when working with Airflow. You can read more about DAGs here: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html</p> <p>The key features of Airflow are:</p> <ul> <li>Webserver: It's a user interface where you can see the status of your jobs, as well as inspect, trigger, and debug your DAGs and tasks. It also gives a database interface and lets you read logs from the remote file store.</li> <li>Scheduler: The Scheduler is a component that monitors and manages all your tasks and DAGs, it checks their status and triggers them in the correct order once their dependencies are complete.</li> <li>Executors: It handles running your task when they are assigned by the scheduler. It can either run the tasks inside the scheduler or push task execution out to workers. Airflow supports a variety of different executors which you can choose between.</li> <li>Metadata database: The metadata database is used by the executor, webserver, and scheduler to store state.</li> </ul> <p></p>"},{"location":"tools-and-applications/airflow/airflow/#installing-apache-airflow-on-openshift","title":"Installing Apache Airflow on OpenShift","text":"<p>Airflow can be run as a pip package, through docker, or a Helm chart. The official Helm chart can be found here: https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-official-airflow-helm-chart</p> <p>See OpenDataHub Airflow - Example Helm Values</p> <p>A modified version of the Helm chart which can be installed on OpenShift 4.12: https://github.com/eformat/openshift-airflow</p>"},{"location":"tools-and-applications/apache-nifi/apache-nifi/","title":"Apache NiFi","text":""},{"location":"tools-and-applications/apache-nifi/apache-nifi/#what-is-it","title":"What is it?","text":"<p>Apache NiFi is an open-source data integration tool that helps automate the flow of data between systems. It is designed to be easy to use and allows users to quickly and efficiently process, transmit, and securely distribute data. NiFi provides a web-based interface for monitoring and controlling data flows, as well as a library of processors for common data manipulation tasks such as filtering, routing, and transformation. It is highly configurable and can be used in a variety of scenarios including data ingestion, ETL, and dataflow management.</p> <p></p> <p>Nifi is a powerful tool to move data between systems and can handle real-time data with ease. It can be used in conjunction with other big data technologies such as Apache Kafka and Apache Spark to create a complete data pipeline. It supports a wide range of protocols and data formats, making it a versatile solution for any organization looking to manage and process large amounts of data.</p>"},{"location":"tools-and-applications/apache-nifi/apache-nifi/#installing-apache-nifi-on-openshift","title":"Installing Apache Nifi on OpenShift","text":"<p>The easiest way to install it is to follow the instructions available on the Nifi on OpenShift repo.</p> <p></p> <p>Contrary to other recipes or the images you can find on the Nifi project, the container images available on this repo are all based on UBI8 and follow OpenShift guidelines and constraints, like running with minimal privileges.</p> <p>Several deployment options are available:</p> <ul> <li>Choice on the number of nodes to deploy,</li> <li>Basic, OIDC or LDAP authentication.</li> </ul>"},{"location":"tools-and-applications/apache-spark/apache-spark/","title":"Apache Spark","text":""},{"location":"tools-and-applications/apache-spark/apache-spark/#what-is-it","title":"What is it?","text":"<p>Apache Spark is an open-source, distributed computing system used for big data processing. It can process large amounts of data quickly and efficiently, and handle both batch and streaming data. Spark uses the in-memory computing concept, which allows it to process data much faster than traditional disk-based systems.</p> <p>Spark supports a wide range of programming languages including Java, Python, and Scala. It provides a number of high-level libraries and APIs, such as Spark SQL, Spark Streaming, and MLlib, that make it easy for developers to perform complex data processing tasks. Spark SQL allows for querying structured data using SQL and the DataFrame API, Spark Streaming allows for processing real-time data streams, and MLlib is a machine learning library for building and deploying machine learning models. Spark also supports graph processing and graph computation through GraphX and GraphFrames.</p>"},{"location":"tools-and-applications/apache-spark/apache-spark/#working-with-spark-on-openshift","title":"Working with Spark on OpenShift","text":"<p>Spark can be fully containerized. Therefore a standalone Spark cluster can of course be installed on OpenShift. However, it sorts of breaks the cloud-native approach brought by Kubernetes of ephemeral workloads. There are in fact many ways to work with Spark on OpenShift, either with Spark-on-Kubernetes operator, or directly through PySpark or spark-submit commands.</p> <p>In this Spark on OpenShift repository, you will find all the instructions to work with Spark on OpenShift.</p> <p>It includes:</p> <ul> <li>pre-built UBI-based Spark images including the drivers to work with S3 storage,</li> <li>instructions and examples to build your own images (to include your own libraries for example),</li> <li>instructions to deploy the Spark history server to gather your processing logs,</li> <li>instructions to deploy the Spark on Kubernetes operator,</li> <li>Prometheus and Grafana configuration to monitor your data processing and operator in real time,</li> <li>instructions to work without the operator, from a Notebook or a Terminal, inside or outside the OpenShit Cluster,</li> <li>various examples to test your installation and the different methods.</li> </ul>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/","title":"Ensemble Models with Triton and KServe","text":"<p>Ensemble models are a feature of the Triton Model Server. They represent a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \u201cdata preprocessing -&gt; inference -&gt; data postprocessing\u201d.</p> <p>Using Ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton.</p> <p>Full reference</p> <p>Note</p> <p>In this repo you will find the full recipe to deploy the Triton runtime with the Single Model Serving Stack, and an example of an Ensemble model to test it.</p> <p>The following instructions are the walkthrough for this installation and test.</p>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/#requirements","title":"Requirements","text":"<p>Deploy Triton as a custom Single Model Serving Runtime in OpenShift AI. You can import a REST Interface version, or a gRPC one.</p> <ul> <li> <p>On the OpenShift AI dashboard, go to Settings-&gt;Serving runtimes</p> <p></p> </li> <li> <p>Add a Servig runtime:</p> <p></p> </li> <li> <p>Select the type:</p> <p></p> </li> <li> <p>Paste the content of the runtime definition:</p> <p></p> </li> <li> <p>You can do the same with the gRPC version:</p> <p></p> </li> <li> <p>You now have the two runtimes available:</p> <p></p> </li> </ul>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/#model-deployment","title":"Model Deployment","text":"<p>This deployment is based on the example model you can find in the model01 of the repo.</p> <ul> <li> <p>Copy the whole content of the model folder (so normally multiple models, plus the Ensemble definition) to an object store bucket.</p> <p></p> </li> <li> <p>In OpenShift AI, create a Data Connection pointing to the bucket.</p> <p></p> </li> <li> <p>Serve the model in OpenShift AI using the custom runtime you imported, pointing it to the data connection.</p> <p> </p> </li> <li> <p>After a few seconds/minutes, the model is served and an inference endpoint is available.</p> <p></p> </li> <li> <p>You can also deploy the gRPC version in the same manner if wou want.</p> </li> </ul>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/#test","title":"Test","text":"<ul> <li> <p>You can use the notebook test-ensemble-rest.ipynb to test the endpoint if you deployed the REST version of the runtime. Another notebook is available for gRPC.</p> <p> </p> </li> </ul>"},{"location":"tools-and-applications/minio/minio/","title":"Minio","text":""},{"location":"tools-and-applications/minio/minio/#what-is-it","title":"What is it?","text":"<p>Minio is a high-performance, S3 compatible object store. It can be deployed on a wide variety of platforms, and it comes in multiple flavors.</p>"},{"location":"tools-and-applications/minio/minio/#why-this-guide","title":"Why this guide?","text":"<p>This guide is a very quick way of deploying the community version of Minio in order to quickly setup a fully standalone Object Store, in an OpenShift Cluster. This can then be used for various prototyping tasks that require Object Storage.</p> <p>Note that nothing in this guide should be used in production-grade environments. Also, Minio is not included in RHOAI, and Red Hat does not provide support for Minio.</p>"},{"location":"tools-and-applications/minio/minio/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Access to an OpenShift cluster</li> <li>Namespace-level admin permissions, or permission to create your own project</li> </ul>"},{"location":"tools-and-applications/minio/minio/#deploying-minio-on-openshift","title":"Deploying Minio on OpenShift","text":""},{"location":"tools-and-applications/minio/minio/#create-a-data-science-project-optional","title":"Create a Data Science Project (Optional)","text":"<p>If you already have your own Data Science Project, or OpenShift project, you can skip this step.</p> <ol> <li>If your cluster already has Red Hat OpenShift AI installed, you can use the Dashboard Web Interface to create a Data Science project.</li> <li>Simply navigate to Data Science Projects</li> <li>And click Create Project</li> <li> <p>Choose a name for your project (here, Showcase) and click Create:</p> <p></p> </li> <li> <p>Make sure to make a note of the Resource name, in case it's different from the name.</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#log-on-to-your-project-in-openshift-console","title":"Log on to your project in OpenShift Console","text":"<ol> <li> <p>Go to your cluster's OpenShift Console:</p> <p></p> </li> <li> <p>Make sure you use the Administrator view, not the developer view.</p> </li> <li> <p>Go to Workloads then Pods, and confirm the selected project is the right one</p> <p></p> </li> <li> <p>You now have a project in which to deploy Minio</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#deploy-minio-in-your-project","title":"Deploy Minio in your project","text":"<ol> <li> <p>Click on the + (\"Import YAML\") button:</p> <p></p> </li> <li> <p>Paste the following YAML in the box, but don't press ok yet!:     <pre><code>---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: minio-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n  volumeMode: Filesystem\n---\nkind: Secret\napiVersion: v1\nmetadata:\n  name: minio-secret\nstringData:\n  # change the username and password to your own values.\n  # ensure that the user is at least 3 characters long and the password at least 8\n  minio_root_user: minio\n  minio_root_password: minio123\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: minio\n    spec:\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: minio-pvc\n      containers:\n        - resources:\n            limits:\n              cpu: 250m\n              memory: 1Gi\n            requests:\n              cpu: 20m\n              memory: 100Mi\n          readinessProbe:\n            tcpSocket:\n              port: 9000\n            initialDelaySeconds: 5\n            timeoutSeconds: 1\n            periodSeconds: 5\n            successThreshold: 1\n            failureThreshold: 3\n          terminationMessagePath: /dev/termination-log\n          name: minio\n          livenessProbe:\n            tcpSocket:\n              port: 9000\n            initialDelaySeconds: 30\n            timeoutSeconds: 1\n            periodSeconds: 5\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio-secret\n                  key: minio_root_user\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio-secret\n                  key: minio_root_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9090\n              protocol: TCP\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: data\n              mountPath: /data\n              subPath: minio\n          terminationMessagePolicy: File\n          image: &gt;-\n            quay.io/minio/minio:RELEASE.2023-06-19T19-52-50Z\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9090\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: minio-service\nspec:\n  ipFamilies:\n    - IPv4\n  ports:\n    - name: api\n      protocol: TCP\n      port: 9000\n      targetPort: 9000\n    - name: ui\n      protocol: TCP\n      port: 9090\n      targetPort: 9090\n  internalTrafficPolicy: Cluster\n  type: ClusterIP\n  ipFamilyPolicy: SingleStack\n  sessionAffinity: None\n  selector:\n    app: minio\n---\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: minio-api\nspec:\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  port:\n    targetPort: api\n  wildcardPolicy: None\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n---\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: minio-ui\nspec:\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  port:\n    targetPort: ui\n  wildcardPolicy: None\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n</code></pre></p> </li> <li> <p>By default, the size of the storage is 20 GB. (see line 11). Change it if you need to.</p> </li> <li>If you want to, edit lines 21-22 to change the default user/password.</li> <li>Press Create.</li> <li> <p>You should see:</p> <p></p> </li> <li> <p>And there should now be a running minio pod:</p> <p></p> </li> <li> <p>As well as  two minio routes:</p> <p></p> </li> <li> <p>The <code>-api</code> route is for programmatic access to Minio</p> </li> <li>The <code>-ui</code> route is for browser-based access to Minio</li> <li>Your Minio Object Store is now deployed, but we still need to create at least one bucket in it, to make it useful.</li> </ol>"},{"location":"tools-and-applications/minio/minio/#creating-a-bucket-in-minio","title":"Creating a bucket in Minio","text":""},{"location":"tools-and-applications/minio/minio/#log-in-to-minio","title":"Log in to Minio","text":"<ol> <li>Locate the minio-ui Route, and open its location URL in a web browser:</li> <li> <p>When prompted, log in</p> <ul> <li>if you kept the default values, then:</li> <li>user: <code>minio</code></li> <li>pass: <code>minio123</code></li> </ul> <p></p> </li> <li> <p>You should now be logged into your Minio instance.</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#create-a-bucket","title":"Create a bucket","text":"<ol> <li> <p>Click on Create a Bucket</p> <p></p> </li> <li> <p>Choose a name for your bucket (for example <code>mybucket</code>) and click Create Bucket:</p> <p></p> </li> <li> <p>Repeat those steps to create as many buckets as you will need.</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#create-a-matching-data-connection-for-minio","title":"Create a matching Data Connection for Minio","text":"<ol> <li> <p>Back in RHOAI, inside of your Data Science Project, Click on Add data connection:</p> <p></p> </li> <li> <p>Then, fill out the required field to match with your newly-deployed Minio Object Storage</p> <p></p> </li> <li> <p>You now have a Data Connection that maps to your mybucket bucket in your Minio Instance.</p> </li> <li>This data connection can be used, among other things<ul> <li>In your Workbenches</li> <li>For your Model Serving</li> <li>For your Pipeline Server Configuration</li> </ul> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#validate","title":"Validate","text":"<p>To test if everything is working correctly, you can access the workbench associated with your Data Connection and run the following commands (i.e., inside a Jupyter notebook): </p> <ol> <li> <p>Install and import MinIO Python Client SDK</p> <p><pre><code>!pip install minio\n</code></pre> <pre><code>from minio import Minio\nfrom minio.error import S3Error\nimport  os\nimport datetime\n</code></pre></p> </li> <li> <p>Access Data Connection properties as environment variables:</p> <pre><code># MinIO client doesn't like URLs with procotol/schema, so use\n# yourendpoint.com instead of https://yourtendpoint.com\nAWS_S3_ENDPOINT = os.getenv(\"AWS_S3_ENDPOINT\")\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_S3_BUCKET = os.getenv(\"AWS_S3_BUCKET\")\n</code></pre> </li> <li> <p>Create the MinIO client</p> <pre><code># Create the MinIO client\nclient = Minio(\n    AWS_S3_ENDPOINT,\n    access_key=AWS_ACCESS_KEY_ID,\n    secret_key=AWS_SECRET_ACCESS_KEY,\n    secure=True  # Set to True if you are using HTTPS\n)\n</code></pre> </li> <li> <p>Test the connection by listing all buckets</p> <pre><code>#List all buckets\ntry:\n    buckets = client.list_buckets()\n    for bucket in buckets:\n        print(bucket.name, bucket.creation_date)\nexcept S3Error as e:\n    print(\"Error occurred: \", e)\n</code></pre> </li> <li> <p>Create a sample local file</p> <pre><code># Create File\nFILE_ON_DISK = 'file.txt'\n\nfile = open(f\"{FILE_ON_DISK}\", \"w\")\nfile.write('Hello there %s recorded at %s.\\n' % (FILE_ON_DISK, datetime.datetime.now()))\nfile.close()\n</code></pre> </li> <li> <p>Upload a file to MinIO</p> <pre><code># Upload a File \nfile_path = FILE_ON_DISK\nobject_name = 'target-file.txt'\n\ntry:\n    client.fput_object(AWS_S3_BUCKET, object_name, file_path)\n    print(f\"'{object_name}' is successfully uploaded as object to bucket '{bucket_name}'.\")\nexcept S3Error as e:\n    print(\"Error occurred: \", e)\n</code></pre> </li> <li> <p>Download a file from MinIO</p> <pre><code># Download a file \nobject_name = 'target-file.txt'\nfile_path = 'file-froms3.txt'\n\ntry:\n client.fget_object(AWS_S3_BUCKET, object_name, file_path)\n print(f\"'{object_name}' is successfully downloaded to '{file_path}'.\")\nexcept S3Error as e:\n print(\"Error occurred: \", e)\n</code></pre> </li> <li> <p>List objects in our bucket</p> <pre><code># Download a file \nobject_name = 'target-file.txt'\nfile_path = 'file-froms3.txt'\n\ntry:\n    client.fget_object(AWS_S3_BUCKET, object_name, file_path)\n    print(f\"'{object_name}' is successfully downloaded to '{file_path}'.\")\nexcept S3Error as e:\n    print(\"Error occurred: \", e)\n</code></pre> </li> </ol> <p>For more complete and detailed information about MinIO Python Client SDK usage, please check the official documentation. </p>"},{"location":"tools-and-applications/minio/minio/#notes-and-faq","title":"Notes and FAQ","text":"<ul> <li>As long as you are using the Route URLs, a Minio running in one namespace can be used by any other application, even running in another namespace, or even in another cluster altogether.</li> </ul>"},{"location":"tools-and-applications/minio/minio/#uninstall-instructions","title":"Uninstall instructions:","text":"<p>This will completely remove Minio and all its content. Make sure you have a backup of the things your need before doing so!</p> <ol> <li> <p>Track down those objects created earlier:</p> <p></p> </li> <li> <p>Delete them all.</p> </li> </ol>"},{"location":"tools-and-applications/mlflow/mlflow/","title":"MLFlow","text":""},{"location":"tools-and-applications/mlflow/mlflow/#what-is-it","title":"What is it?","text":"<p>MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. MLflow currently offers four components:  Read more here: https://mlflow.org/</p>"},{"location":"tools-and-applications/mlflow/mlflow/#helm-installation-into-openshift-namespace","title":"Helm installation into OpenShift namespace","text":""},{"location":"tools-and-applications/mlflow/mlflow/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Install the \"Crunchy Postgres for Kubernetes\" operator (can be found in OperatorHub) - To store the MLFlow config</li> <li>Install the \"OpenShift Data Foundation\" operator (can be found in OperatorHub) - To provide S3 storage for the experiments and models</li> </ul>"},{"location":"tools-and-applications/mlflow/mlflow/#install","title":"Install","text":"<pre><code>&lt;Create an OpenShift project, either through the OpenShift UI or 'oc new-project project-name'&gt;\nhelm repo add strangiato https://strangiato.github.io/helm-charts/\nhelm repo update\n&lt;Log in to the correct OpenShift project through 'oc project project-name'&gt;\nhelm upgrade -i mlflow-server strangiato/mlflow-server\n</code></pre>"},{"location":"tools-and-applications/mlflow/mlflow/#additional-options","title":"Additional Options","text":"<p>The MLFlow Server helm chart provides a number of customizable options when deploying MLFlow.  These options can be configured using the <code>--set</code> flag with <code>helm install</code> or <code>helm upgrade</code> to set options directly on the command line or through a <code>values.yaml</code> file using the <code>--values</code> flag.</p> <p>For a full list of configurable options, see the helm chart documentation:</p> <p>https://github.com/strangiato/helm-charts/tree/main/charts/mlflow-server#values</p>"},{"location":"tools-and-applications/mlflow/mlflow/#opendatahub-dashboard-application-tile","title":"OpenDataHub Dashboard Application Tile","text":"<p>As discussed in the Dashboard Configuration, ODH/RHOAI allows administrators to add a custom application tile for additional components on the cluster.</p> <p></p> <p>The MLFlow Server helm chart supports creation of the Dashboard Application tile as a configurable value.  If MLFlow Server is installed in the same namespace as ODH/RHOAI you can install the dashboard tile run the following command:</p> <pre><code>helm upgrade -i mlflow-server strangiato/mlflow-server \\\n    --set odhApplication.enabled=true\n</code></pre> <p>The MLFlow Server helm chart also supports installing the odhApplication object in a different namespace, if MLFlow Server is not installed in the same namespace as ODH/RHOAI:</p> <pre><code>helm upgrade -i mlflow-server strangiato/mlflow-server \\\n    --set odhApplication.enabled=true \\\n    --set odhApplication.namespaceOverride=redhat-ods-applications\n</code></pre> <p>After enabling the odhApplication component, wait 1-2 minutes and the tile should appear in the Explorer view of the dashboard.</p> <p>Note</p> <p>This feature requires ODH v1.4.1 or newer</p>"},{"location":"tools-and-applications/mlflow/mlflow/#test-mlflow","title":"Test MLFlow","text":"<ul> <li>Go to the OpenShift Console and switch to Developer view.</li> <li>Go to the Topology view and make sure that you are on the MLFlow project.</li> <li>Check that the MLFlow circle is dark blue (this means it has finished deploying).</li> <li>Press the \"External URL\" link in the top right corner of the MLFlow circle to open up the MLFlow UI.</li> <li>Run <code>helm test mlflow-server</code> in your command prompt to test MLFlow. If successful, you should see a new experiment called \"helm-test\" show up in the MLFlow UI with 3 experiments inside it.</li> </ul>"},{"location":"tools-and-applications/mlflow/mlflow/#adding-mlflow-to-training-code","title":"Adding MLFlow to Training Code","text":"<pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\n\n# Set tracking URI\nmlflow.set_tracking_uri(\u201chttps://&lt;route-to-mlflow&gt;\u201d)\n\n# Setting the experiment\nmlflow.set_experiment(\"my-experiment\")\n\nif __name__ == \"__main__\":\n    # Enabling automatic logging for scikit-learn runs\n    mlflow.sklearn.autolog()\n\n    # Starting a logging run\n    with mlflow.start_run():\n        # train\n</code></pre>"},{"location":"tools-and-applications/mlflow/mlflow/#source-code","title":"Source Code","text":"<p>MLFlow Server Source Code: https://github.com/strangiato/mlflow-server</p> <p>MLFlow Server Helm Chart Source Code: https://github.com/strangiato/helm-charts/tree/main/charts/mlflow-server</p>"},{"location":"tools-and-applications/mlflow/mlflow/#demos","title":"Demos","text":"<ul> <li>Credit Card Fraud Detection pipeline using MLFlow together with RHOAI: Demo</li> </ul>"},{"location":"tools-and-applications/rclone/rclone/","title":"Rclone","text":""},{"location":"tools-and-applications/rclone/rclone/#what-is-it","title":"What is it?","text":"<p>Rclone is a program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business &amp; consumer file storage services, as well as standard transfer protocols.</p> <p>Users call rclone \"The Swiss army knife of cloud storage\", and \"Technology indistinguishable from magic\".</p> <p>Rclone really looks after your data. It preserves timestamps and verifies checksums at all times. Transfers over limited bandwidth; intermittent connections, or subject to quota can be restarted, from the last good file transferred. You can check the integrity of your files. Where possible, rclone employs server-side transfers to minimize local bandwidth use and transfers from one provider to another without using local disk.</p> <p>Rclone is mature, open-source software originally inspired by rsync and written in Go. The friendly support community is familiar with varied use cases.</p> <p>The implementation described here is a containerized version of Rclone to run on OpenShift, alongside or integrated within ODH/RHOAI.</p>"},{"location":"tools-and-applications/rclone/rclone/#deployment","title":"Deployment","text":""},{"location":"tools-and-applications/rclone/rclone/#integrated-in-open-data-hub-or-openshift-ai","title":"Integrated in Open Data Hub or OpenShift AI","text":"<p>Use this method if you want to use Rclone from the ODH/RHOAI launcher or in a Data Science Project.</p> <ul> <li>In the Cluster Settings menu, import the image <code>quay.io/guimou/rclone-web-openshift:odh-rhoai_latest</code>. You can name it Rclone. </li> <li>In your DSP project, create a new workbench using the Rclone image. You can set the storage size as minimal as it's only there to store the configuration of the endpoints.  </li> </ul> <p>Tip</p> <p>The minimal size allowed by the dashboard for a storage volume is currently 1GB, which is way more than what is required for the Rclone configuration. So you can also create a much smaller PVC manually in the namespace corresponding to your Data Science Project, for example 100MB or less, and select this volume when creating the workbench.</p> <ul> <li>Launch Rclone from the link once it's deployed! </li> <li>After the standard authentication, you end up on the Rclone Login page. There is nothing to enter, but I have not found yet how to bypass it. So simply click on \"Login\". </li> </ul>"},{"location":"tools-and-applications/rclone/rclone/#standalone-deployment","title":"Standalone deployment","text":"<p>Use this method if you want to use Rclone on its own in a namespace. You can still optionally make a shortcut appear in the ODH/RHOAI dashboard.</p> <ul> <li>Create a project/namespace for your installation.</li> <li>Clone or head to this repo.</li> <li>From the deploy folder, apply the different YAML files:<ul> <li>01-pvc.yaml: creates a persistent volume to hold the configuration</li> <li>02-deployment.yaml: creates the deployment. Modify admin account and password if you want to restrict access. You should!</li> <li>03-service.yaml, 04-route.yaml: create the external access so that you can connect to the Web UI.</li> <li>Optionally, to create a tile on the ODH/RHOAI dashboard:<ul> <li>modify the 05-tile.yaml file with the address of the Route that was created previously (namespace and name of the Route object).</li> <li>the will appear under the available applications in the dashboard. Select it and click on \"Enable\" to make it appear in the \"Enabled\" menu.</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools-and-applications/rclone/rclone/#configuration","title":"Configuration","text":"<p>In this example, we will create an S3 configuration that connects to a bucket on the MCG from OpenShift Data Foundation. So you must have created this bucket in advance and have all the information about it: endpoint, access and secret keys, bucket name.</p> <ul> <li>In Rclone, click on \"Configs\" to create the new Remote. </li> <li>Create new configuration, give it a name, and select \"Amazon S3 Compliant Storage Providers...\", which includes Ceph and MCG (even if not listed). </li> <li>Enter the connection info. You only have to enter the Access key and Secret, as well as the Endpoint in \"Endpoint for S3 API\". This last info is automatically copied in other fields, that's normal. </li> <li>Finalize the config by clicking on \"Next\" at the bottom.</li> </ul> <p>Now that you have the Remote set up, you can go on the Explorer, select the Remote, and browse it!  </p>"},{"location":"tools-and-applications/rclone/rclone/#usage-example","title":"Usage Example","text":"<p>In this simple example, we will transfer a dump sample from Wikipedia. Wikimedia publishes those dumps daily, and they are mirrored by different organizations. In a \"standard\" setup, loading those information into your object store would not be really practical, sometimes involving downloading it first locally to then push it to your storage.</p> <p>This is how we can do it with Rclone.</p> <ul> <li>Create your Bucket Remote as described in Configuration.</li> <li>Create another remote of type \"HTTP\", and enter the address of one of the mirrors. Here I used <code>https://dumps.wikimedia.your.org/wikidatawiki/</code>.</li> <li>Open the Explorer view, set it in dual-pane layout. In the first pane open your Bucket Remote, and in the other one the HTTP. This is what it will look like: </li> <li>Browse to the folder you want, select a file or a folder, and simply drag and drop it from the Wikidump to your bucket. You can select a big one to make things more interesting!</li> <li>Head for the dashboard where you will see the file transfer happening in the background. </li> </ul> <p>That's it! Nothing to install, high speed optimized transfer, and you could even do multiple transfers in the background,...</p>"},{"location":"tools-and-applications/riva/riva/","title":"NVIDIA RIVA","text":"<p>NVIDIA\u00ae Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance.</p> <p>Riva offers pretrained speech models in NVIDIA NGC\u2122 that can be fine-tuned with the NVIDIA NeMo on a custom data set, accelerating the development of domain-specific models by 10x.</p> <p>Models can be easily exported, optimized, and deployed as a speech service on premises or in the cloud with a single command using Helm charts.</p> <p>Riva\u2019s high-performance inference is powered by NVIDIA TensorRT\u2122 optimizations and served using the NVIDIA Triton\u2122 Inference Server, which are both part of the NVIDIA AI platform.</p> <p>Riva services are available as gRPC-based microservices for low-latency streaming, as well as high-throughput offline use cases.</p> <p>Riva is fully containerized and can easily scale to hundreds and thousands of parallel streams.</p>"},{"location":"tools-and-applications/riva/riva/#deployment","title":"Deployment","text":"<p>The guide to deploy Riva on Kubernetes has to be adapted for OpenShift. Here are the different steps.</p>"},{"location":"tools-and-applications/riva/riva/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have access and are logged into NVIDIA NGC. For step-by-step instructions, refer to the NGC Getting Started Guide. Specifically you will need your API Key from NVIDIA NGC.</li> <li>You have at least one worker node with an NVIDIA Volta\u2122, NVIDIA Turing\u2122, or an NVIDIA Ampere architecture-based GPU. For more information, refer to the Support Matrix.</li> <li>The Node Feature Discovery and the NVIDIA operators have been properly installed and configured on your OpenShift Cluster to enable your GPU(s). Full instructions here</li> <li>The Pod that will be deployed will consume about 10GB of RAM. Make sure you have enough resources on your node (on top of the GPU itself), and you don't have limits in place that would restrict this. GPU memory consumption will be about 12GB with all models loaded.</li> </ol>"},{"location":"tools-and-applications/riva/riva/#installation","title":"Installation","text":"<p>Included in the NGC Helm Repository is a chart designed to automate deployment to a Kubernetes cluster. This chart must be modified for OpenShift.</p> <p>The Riva Speech AI Helm Chart deploys the ASR, NLP, and TTS services automatically. The Helm chart performs a number of functions:</p> <ul> <li>Pulls Docker images from NGC for the Riva Speech AI server and utility containers for downloading and converting models.</li> <li>Downloads the requested model artifacts from NGC as configured in the values.yaml file.</li> <li>Generates the Triton Inference Server model repository.</li> <li>Starts the Riva Speech AI server as configured in a Kubernetes pod.</li> <li>Exposes the Riva Speech AI server as a configured service.</li> </ul> <p>Examples of pretrained models are released with Riva for each of the services. The Helm chart comes preconfigured for downloading and deploying all of these models.</p> <p>Installation Steps:</p> <ol> <li> <p>Download the Helm chart</p> <pre><code>export NGC_API_KEY=&lt;your_api_key&gt;\nhelm fetch https://helm.ngc.nvidia.com/nvidia/riva/charts/riva-api-2.11.0.tgz \\\n        --username=\\$oauthtoken --password=$NGC_API_KEY --untar\n</code></pre> </li> <li> <p>Switch to the newly created folder, <code>riva-api</code></p> </li> <li> <p>In the <code>templates</code> folder, modify the file <code>deployment.yaml</code>. For both the container <code>riva-speech-api</code> and the initContainer <code>riva-model-init</code> you must add the following security context information:</p> <pre><code>securityContext:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop: [\"ALL\"]\n    seccompProfile:\n      type: \"RuntimeDefault\"\n    runAsNonRoot: true\n</code></pre> </li> <li> <p>The file <code>deployment.yaml</code> should now look like this:</p> <pre><code>...\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ template \"riva-server.fullname\" . }}\n  ...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n        - name: riva-speech-api\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop: [\"ALL\"]\n            seccompProfile:\n              type: \"RuntimeDefault\"\n            runAsNonRoot: true\n          image: {{ $server_image }}\n          ...\n      initContainers:\n        - name: riva-model-init\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop: [\"ALL\"]\n            seccompProfile:\n              type: \"RuntimeDefault\"\n            runAsNonRoot: true\n          image: {{ $servicemaker_image }}\n          ...\n</code></pre> </li> <li> <p>At the root of <code>riva-api</code>, modify the file <code>values.yaml</code>:</p> <ol> <li> <p>You will need to convert your API Key to a password value. In a Terminal run:</p> <pre><code>echo -n $NGC_API_KEY | base64 -w0\n</code></pre> </li> <li> <p>In the <code>ngcCredentials</code> section ov <code>values.yaml</code>, enter the password you obtained above and your email</p> </li> <li>In the <code>modelRepoGenerator</code> section, for the <code>modelDeployKey</code> value, enter <code>dGx0X2VuY29kZQ==</code>. (This value is obtained from the command <code>echo -n tlt_encode | base64 -w0</code>.</li> <li>In the <code>persistentVolumeClaim</code> section, set <code>usePVC</code> to true. This is very important as it will disable the hostPath configuration for storage that is not permitted by default on OpenShift.</li> <li>If you don't have a storageClass set as default, or want to you another one, enter the name of the class you want to use in <code>storageClassName</code>. Otherwise leave this field empty and the default class will be used.</li> <li>Optionally, modify the storageSize.</li> <li>Leave the <code>ingress</code> section as is, we will create an OpenShift Route later.</li> <li>Optionally you can modify other values in the file to enable/disable certain models, or modify their configuration.</li> </ol> </li> <li> <p>Log into your OpenShift cluster from a Terminal, and create a project <code>riva-api</code>:</p> <pre><code>oc new-project riva-api\n</code></pre> </li> <li> <p>Move up one folder (so outside of the <code>riva-api</code> folder), and install NVIDIA Riva with the modified Helm chart:</p> <pre><code>helm install riva-api riva-api\n</code></pre> </li> </ol> <p>The deployment will now start.</p> <p>Info</p> <p>Beware that the deployment can be really long the first time, about 45mn if you have all the models and features selected. Containers and models have to be downloaded and configured. Please be patient...</p>"},{"location":"tools-and-applications/riva/riva/#usage","title":"Usage","text":"<p>The Helm chart had automatically created a Service, <code>riva-api</code> in the namespace where you have deployed it. If you followed this guide, this should also be <code>riva-api</code>. So within the OpenShift cluster, the API is accessible at <code>riva-api.riva-api.svc.cluster.local</code>.</p> <p>Different ports are accessible:</p> <ul> <li>http (8000): HTTP port of the Triton server.</li> <li>grpc (8001): gRPC port of the Triton server.</li> <li>metrics (8002): port for the metrics of the Triton server.</li> <li>speech-grpc (50051): gRPC port of the Speech that exposes directly the different services you can use. This is normally the one that you will use.</li> </ul> <p>If you want to use the API outside of the OpenShift cluster, you will have to create one or multiple Routes to those different endpoints.</p>"},{"location":"tools-and-applications/riva/riva/#example","title":"Example","text":"<ul> <li>On the same cluster where NVIDIA Riva is deployed, deploy RHOAI or ODH and launch a Notebook (Standard DataScience is enough).</li> <li>Clone the NVIDIA Riva tutorials repository at https://github.com/nvidia-riva/tutorials</li> <li>Open a Terminal and install the client with <code>pip install nvidia-riva-client</code>:</li> </ul> <p>(depending on the base image you used this may yield errors that you can ignore most of times).</p> <ul> <li>In the <code>tutorials</code> folder, open the notebook <code>asr-basics.ipynb</code>.</li> <li>In the cell that defines the uri of the API server, modify the default (localhost) for the address of the API server: <code>riva-api.riva-api.svc.cluster.local</code></li> </ul> <p></p> <ul> <li>Run the notebook!</li> </ul> <p></p> <p>Note</p> <p>In this example, only the first part of the notebook will work as only the English models have been deployed. You would have to adapt the configuration for other languages.</p>"},{"location":"whats-new/whats-new/","title":"What's new?","text":"<p>2024-01-03: Add Ensemble models demo.</p> <p>2024-03-28: Add Zero to Workbench documentation.</p> <p>2024-03-04: Refactoring of the site. Update on the LLMs deployment and RAG demo. Single Stack Serving certificate how-to.</p> <p>2023-12-21: Name change for Red Hat OpenShift Data Science, which becomes Red Hat OpenShift AI.</p> <p>2023-10-16: Add documentation for LLMs deployment and RAG demo.</p> <p>2023-08-01: Update to Spark documentation to include usage without the operator Tools and Applications-&gt;Apache Spark</p> <p>2023-07-05: Add documentation on Time Slicing and Autoscaling for NVIDIA GPUs ODH/RHOAI How-Tos-&gt;NVIDIA GPUs</p> <p>2023-07-05: New example of how to configure a Custom Serving Runtime with Triton.</p> <p>2023-07-03: New Minio tutorial on how to quickly deploy a simple Object Storage inside your OpenShift Project, for quick prototyping.</p> <p>2023-06-30: New NVIDIA GPU installation documentation with Node tainting in ODH/RHOAI How-Tos-&gt;NVIDIA GPUs</p> <p>2023-06-02: NVIDIA Riva documentation in Tools and Applications-&gt;NVIDIA Riva</p> <p>NVIDIA\u00ae Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance.</p> <p>2023-02-06: Rclone documentation in Tools and Applications-&gt;Rclone.</p> <p>Rclone is a program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business &amp; consumer file storage services, as well as standard transfer protocols.</p> <p>2023-02-02: Addition of VSCode and RStudio images to custom workbenches.</p> <p>2023-01-22: Addition of StarProxy to Patterns-&gt;Starburst/Trino Proxy.</p> <p>Starproxy is a fully HTTP compliant proxy that is designed to sit between clients and a Trino/Starburst cluster.</p>"}]}